[["index.html", "Climate Change 1 Climate Change 1.1 Neo-liberal Climate Change", " Climate Change Dyrehaugen Web Notebook 2021-04-26 1 Climate Change 1.1 Neo-liberal Climate Change The neoliberal solution to climate change is to hope that somehow it will become profitable to save the planet. This will not work. ((ExistentialComics?)) "],["climate-models.html", "2 Climate Models 2.1 DICE Climate Model 2.2 EZ-Climate Model 2.3 FAIR Climate Model 2.4 Global Calculator 2.5 Monash Climate Model 2.6 NorESM 2.7 Goal Index 2.8 Model Drift 2.9 Spatial Shock 2.10 Model-evaluation", " 2 Climate Models Overview text on selected Climate Models 2.1 DICE Climate Model Independent of the normative assumptions of inequality aversion and time preferences, the Paris agreement constitutes the economically optimal policy pathway for the century. Authors claim they show this by incorporating a damage-cost curve reproducing the observed relation between temperature and economic growth into the integrated assessment model DICE. Glanemann (2021) DICE Paris CBA (pdf) 2.1.1 The failure of Dice Economics If there is one climate economist who is respected above all others, it’s William Nordhaus of Yale, who won the Econ Nobel in 2018 “for integrating climate change into long-run macroeconomic analysis.” The prize specifically cited Nordhaus’ creation of an “integrated assessment model” for analyzing the costs of climate change. The most famous of these is the DICE Model, used by the Environmental Protection Agency. But the DICE Model, or at least the version we’ve been using for years, is obviously bananas. Noah Smith For other economists look here Catie Hausman Twitter Thread 2.2 EZ-Climate Model Some text on EZClimate 2.3 FAIR Climate Model The FAIR model satisfies all criteria set by the NAS for use in an SCC calculation. 22 Importantly, this model generates projections of future warming that are consistent with comprehensive, state- of-the-art models and it can be used to accurately characterize current best understanding of the uncertainty regarding the impact that an additional ton of CO 2 has on global mean surface temperature (GMST). Finally, FAIR is easily implemented and transparently documented, 23 and is already being used in updates of the SCC. 24 A key limitation of FAIR and other simple climate models is that they do not represent the change in global mean sea level rise (GMSL) due to a marginal change in emissions. Carleton Greenstone (2021) Updating SCC (pdf) FAIR 2.4 Global Calculator Used by Kuhnhenn - STS - Heinrch Böll Stiftung GlobalCalculator Global Calculator Tool 2.5 Monash Climate Model Some text on Monash Model 2.6 NorESM Norwegian Earth System Model About A climate model solves mathematically formulated natural laws on a three-dimensional grid. The climate model divides the soil system into components (atmosphere, sea, sea ice, land with vegetation, etc.) that interact through transmission of energy, motion and moisture. When the climate model also includes advanced interactive atmosphere chemistry and biogeochemical cycles (such as the carbon cycle), it is called an earth system model. The Norwegian Earth System Model NorESM has been developed since 2007 and has been an important tool for Norwegian climate researchers in the study of the past, present and future climate. NorESM has also contributed to climate simulation that has been used in research assessed in the IPCC’s fifth main report. INES The project Infrastructure for Norwegian Earth System Modeling (INES) will support the further development of NorESM and help Norwegian scientists also gain access to a cutting-edge earth system model in the years to come. Technical support will be provided for the use of a more refined grid, the ability to respond to climate change up to 10 years in advance, the inclusion of new processes at high latitudes and the ability of long-term projection of sea level. Climate simulations with NorESM are made on some of the most powerful supercomputers in Norway, and INES will help these exotic computers to be exploited in the best possible way and that the large data sets produced are efficiently stored and used. The project will ensure that researchers can efficiently use the model tool, analyze results and make the results available. 2.6.1 CCSM4 UCAR NCAR The University Corporation for Atmospheric Research (UCAR) is a US nonprofit consortium of more than 100 colleges and universities providing research and training in the atmospheric and related sciences. UCAR manages the National Center for Atmospheric Research (NCAR) and provides additional services to strengthen and support research and education through its community programs. Its headquarters, in Boulder, Colorado, include NCAR’s Mesa Laboratory. (Wikipedia) CCSM The Community Climate System Model (CCSM) is a coupled climate model for simulating Earth’s climate system. CCSM consists of five geophysical models: atmosphere (atm), sea-ice (ice), land (lnd), ocean (ocn), and land-ice (glc), plus a coupler (cpl) that coordinates the models and passes information between them. Each model may have “active,” “data,” “dead,” or “stub” component version allowing for a variety of “plug and play” combinations. During the course of a CCSM run, the model components integrate forward in time, periodically stopping to exchange information with the coupler. The coupler meanwhile receives fields from the component models, computes, maps, and merges this information, then sends the fields back to the component models. The coupler brokers this sequence of communication interchanges and manages the overall time progression of the coupled system. A CCSM component set is comprised of six components: one component from each model (atm, lnd, ocn, ice, and glc) plus the coupler. Model components are written primarily in Fortran 90. ccsm4 CESM The Community Earth System Model (CESM) is a fully-coupled, global climate model that provides state-of-the-art computer simulations of the Earth’s past, present, and future climate states. CESM2 is the most current release and contains support for CMIP6 experiment configurations. cesm models Simpler Models As part of CESM2.0, several dynamical core and aquaplanet configurations have been made available. Simpler Models 2.6.2 NorESM Features Despite the nationally coordinated effort, Norway has insufficient expertise and manpower to develop, test, verify and maintain a complete earth system model. For this reason, NorESM is based on the Community Climate System Model version 4, CCSM4, operated at the National Center for Atmospheric Research on behalf of the Community Climate System Model (CCSM)/Community Earth System Model (CESM) project of the University Corporation for Atmospheric Research. NorESM is, however, more than a model “dialect” of CCSM4. Notably, NorESM differs from CCSM4 in the following aspects: NorESM utilises an isopycnic coordinate ocean general circulation model developed in Bergen during the last decade originating from the Miami Isopycnic Coordinate Ocean Model (MICOM). The atmospheric module is modified with chemistry–aerosol–cloud–radiation interaction schemes developed for the Oslo version of the Community Atmosphere Model (CAM4-Oslo). Finally, the HAMburg Ocean Carbon Cycle (HAMOCC) model developed at the Max Planck Institute for Meteorology, Hamburg, adapted to an isopycnic ocean model framework, constitutes the core of the biogeochemical ocean module in NorESM. In this way NorESM adds to the much desired climate model diversity, and thus to the hierarchy of models participating in phase 5 of the Climate Model Intercomparison Project (CMIP5). In this and in an accompanying paper (Iversen et al., 2013), NorESM without biogeochemical cycling is presented. The reader is referred to Assmann et al. (2010) and Tjiputra et al. (2013) for a description of the biogeochemical ocean component and carbon cycle version of NorESM, respectively. There are several overarching objectives underlying the development of NorESM. Western Scandinavia and the surrounding seas are located in the midst of the largest surface temperature anomaly on earth governed by anomalously large oceanic and atmospheric heat transports. Small changes to these transports may result in large and abrupt changes in the local climate. To better understand the variability and stability of the climate system, detailed studies of the formation, propagation and decay of thermal and (oceanic) fresh water anomalies are required. NorESM is, as mentioned above, largely based on CCSM4. The main differences are the isopycnic coordinate ocean module in NorESM and that CAM4-Oslo substitutes CAM4 as the atmosphere module. The sea ice and land models in NorESM are basically the same as in CCSM4 and the Com- munity Earth System Model version 1 (CESM1), except that deposited soot and mineral dust aerosols on snow and sea ice are based on the aerosol calculations in CAM4-Oslo. 2.6.2.1 NorESM Aerosol Interactions The aerosol module is extended from earlier versions that have been published, and includes life-cycling of sea salt, mineral dust, particulate sulphate, black carbon, and primary and secondary organics. The impacts of most of the numer- ous changes since previous versions are thoroughly explored by sensitivity experiments. The most important changes are: modified prognostic sea salt emissions; updated treatment of precipitation scavenging and gravitational settling; inclu- sion of biogenic primary organics and methane sulphonic acid (MSA) from oceans; almost doubled production of land- based biogenic secondary organic aerosols (SOA); and in- creased ratio of organic matter to organic carbon (OM/OC) for biomass burning aerosols from 1.4 to 2.6. Compared with in situ measurements and remotely sensed data, the new treatments of sea salt and dust aerosols give smaller biases in near-surface mass concentrations and aerosol optical depth than in the earlier model version. The model biases for mass concentrations are approximately un- changed for sulphate and BC. The enhanced levels of mod- led OM yield improved overall statistics, even though OM is still underestimated in Europe and overestimated in North America. The global anthropogenic aerosol direct radiative forc- ing (DRF) at the top of the atmosphere has changed from a small positive value to −0.08 W m −2 in CAM4-Oslo. The sensitivity tests suggest that this change can be attributed to the new treatment of biomass burning aerosols and gravita- tional settling. Although it has not been a goal in this study, the new DRF estimate is closer both to the median model estimate from the AeroCom intercomparison and the best es- timate in IPCC AR4. Estimated DRF at the ground surface has increased by ca. 60 %, to −1.89 W m −2 The increased abundance of natural OM and the introduc- tion of a cloud droplet spectral dispersion formulation are the most important contributions to a considerably decreased es- timate of the indirect radiative forcing (IndRF). The IndRF is also found to be sensitive to assumptions about the coat- ing of insoluble aerosols by sulphate and OM. The IndRF of −1.2 W m −2 , which is closer to the IPCC AR4 estimates than the previous estimate of −1.9 W m −2 , has thus been obtained without imposing unrealistic artificial lower bounds on cloud droplet number concentrations. NorESM Bentsen (2013) NorESM - Part 1 (pdf) Iversen (2013) NorESM - Part 2 (pdf) Assmann (2010) Biogeochemical Ocean Component - Isopycnic (pdf) Tjiputra (2010) Carbon Cycle Component (pdf) Kirkevaag (2013) Aerosol-Climate Interactions (pdf) Community Earth System Model CESM 2.7 Goal Index In economic modelling choice of goal index (utility) function matters. Daniel 20181 presents this figure: Fig. Optimal CO2-prices with increasing risk aversion for EZ vs CRRA utility specification. (From Daniel 2018) As one of the co-authors explain: ‘We where not able to get the Social Cost of Carbon (SCC) under $120.’ That is for ‘reasonable risk aversion,’ using EZ-utilities. The ‘standard’ specification - with CRRA - utilities ends up with SCC of $20 or below. \\[V_1 = A [\\tilde{C\\_t}, \\mu_t(V\\_{t+1})]\\] Specification of the Goal Index function may seem a trivial technical issue - no so! There exists a broad professional litterature and profound discussions on this matter - which might de difficult to dis-entangle. Let us begin with Frank Ramsey’s growth model from 1928, commonly known as the Ramsey-Cass-Koopmans model. \\(F(K,L)\\) is an aggregate production function with factors \\(K\\) (Capital) and \\(L\\) (Labour). 2.8 Model Drift Abstract Sausen A method is proposed for removing the drift of coupled atmosphere-ocean models, which in the past has often hindered the application of coupled models in climate response and sensitivity experiments. The ocean-atmosphere flux fields exhibit inconsistencies when evaluated separately for the individual sub-systems in independent, uncoupled mode equilibrium climate computations. In order to balance these inconsistencies a constant ocean-atmosphere flux correction field is introduced in the boundary conditions coupling the two sub-systems together. The method ensures that the coupled model operates at the reference climate state for which the individual model subsystems were designed without affecting the dynamical response of the coupled system in climate variability experiments. The method is illustrated for a simple two component box model and an ocean general circulation model coupled to a two layer diagnostic atmospheric model. Memo Barthel The coupling of different climate sub-systems poses a number of technical problems. An obvious problem arising from the different time scales is the synchronization or matching of the numerical integration of subsys- tems characterized by widely differing time steps. A more subtle problem is Model Drift When two general circulation models of the atmosphere and ocean are coupled together in a single model, it is generally found that the cou- pled system gradually drifts into a new climate equilibrium state which is far removed from the observed climate. The coupled model climate equilibrium may be so unrealistic (for example, with respect to sea ice extent, or the oceanic equa- torial current system) that climate response or sensitivity experiments relative to this state be- come meaningless. This occurs regularly even when the individual models have been carefully tested in detailed numerical experiments in the decoupled mode and have been shown to yield satisfactory simulations of the climate of the sepa- rate ocean or atmosphere sub-systems. The drift of the coupled model is clearly a sign that something is amiss with the models. Howev- er, we suggest that it is not necessary to wait with climate response and sensitivity experiments with coupled models unit all causes of model drift have been properly identified and removed. Model drift is, in fact, an extremely sensitive indi- cator of model imperfections. The fact that the equilibrium climate into which a coupled model drifts is unacceptably far removed from the real climate does not necessarily imply that the model dynamics are too unrealistic for the model to be applied for climate response and sensitivity ex- periments. One should therefore devise methods for separating the drift problem from the basically independent problem of determining the change of the simulated climate induced by a change in boundary conditions a n d / o r external forcing (cli- mate response), and from the question of the ef- fect of changes in the physical or numerical for- mulation of the model (model sensitivity). Flux Correction The separation of the mean climate simulation from the climate response or sensitiv- ity problem can be achieved for coupled models rather simply by an alternative technique, the flux correction method. The errors that result in a drift of the coupled model are compensated in this method by con- stant correction terms in the flux expressions by which the separate sub-system models are cou- pled together. The correction terms have no in- fluence on the dynamics of the system in climate response or sensitivity experiments, but ensure that the “working point” of the model lies close to the climate state for which the individual models were originally tuned. The basic principle of the flux correction method is to couple the atmosphere and the ocean in such a manner that in the unperturbed case each sub- system simulates its own mean climate in the same manner as in the uncoupled mode, but re- sponds fully interactively to the other sub-system in climate response or sensitivity experiments. Sausen (1988) Coupled Ocean-Atmosphere Model Drift Flux Correction (pdf) 2.9 Spatial Shock 2.9.1 Coastal Flooding Desmet Just moving to higher grounds Nobel Prize winner William Nordhaus has called climate change “the ultimate challenge for economics.” Economists increasingly have been trying to understand how rising tides and global temperatures will impact resource allocation around the globe, as well as the potential policy tools that can help curb damage to our natural world. SMU professor Klaus Desmet says that a lot of those analyses are missing a critical factor: migration. Desmet coauthored a paper in the American Economic JournalEvaluating the Economic Cost of Coastal Flooding (paywall): Macroeconomics that examines how economic output will be affected over the next 200 years as humans move away from coastal areas threatened by rising sea levels. Although losses in vulnerable Southeast Asian cities such as Bangkok and Shanghai will still be very significant, their research shows that overall GDP declines are substantially less than predicted by models that don’t account for spatial shifts in economic activity. Climate change is to a large extent a spatial shock. Migration as one of the key responses to climate change. What we find in our model is that at a global level, flooding decreases real GDP by about 0.1 percent by the year 2200. If we were to completely ignore the dynamic spatial response of the economy, if we were to have everyone stay put in the face of rising seas, then the loss would actually increase to 4.5 percent. So, that difference between 0.1 and 4.5 percent underscores the first-order importance of taking into account moving, migration, and the spatial dynamic response of the economy to rising sea levels. We develop a high resolution dynamic model of the world economy. This model splits up the world into 64,800 1° by 1° grid cells, which are linked to each other through trade and migration. We feed into this model high-quality projections of both global and local sea level rise over the next 200 years. . . . When we run our model forward, we can then assess what the economic effect will be of having these pieces of land lost for production. We find large losses in coastal areas of south and east Asia, with countries such as Vietnam, Thailand, and Bangladesh losing up to 10 percent of real GDP in present discounted value terms over the next 200 years. Other areas that will also suffer disproportionately include coastal Northwest Europe, [and] some areas on the US East Coast and Gulf Coast. By contrast, the Pacific Coast of the Americas is much less affected and, in fact, most of the coastal areas of Africa are also a lot less affected. Desmet 2.10 Model-evaluation GCMeval: a tool for climate model ensemble evaluation The global climate models indicate quite a range of future outcomes in terms of precipitation and temperature. To account for that, regional scenarios need to use fairly large multi-model ensembles. GCM-eval 2.10.1 Measuering Forcings Earth is on a budget – an energy budget. Our planet is constantly trying to balance the flow of energy in and out of Earth’s system. But human activities are throwing that off balance, causing our planet to warm in response. Adding more components that absorb radiation – like greenhouse gases – or removing those that reflect it – like aerosols – throws off Earth’s energy balance, and causes more energy to be absorbed by Earth instead of escaping into space. This is called a radiative forcing, and it’s the dominant way human activities are affecting the climate. limate modelling predicts that human activities are causing the release of greenhouse gases and aerosols that are affecting Earth’s energy budget. Now, a NASA study has confirmed these predictions with direct observations for the first time: radiative forcings are increasing due to human actions, affecting the planet’s energy balance and ultimately causing climate change. The paper was published online March 25, 2021, in the journal Geophysical Research Letters. NASA’s Clouds and the Earth’s Radiant Energy System (CERES) project studies the flow of radiation at the top of Earth’s atmosphere. A series of CERES instruments have continuously flown on satellites since 1997. Each measures how much energy enters Earth’s system and how much leaves, giving the overall net change in radiation. That data, in combination with other data sources such as ocean heat measurements, shows that there’s an energy imbalance on our planet. But it doesn’t tell us what factors are causing changes in the energy balance. This study used a new technique to parse out how much of the total energy change is caused by humans. The researchers calculated how much of the imbalance was caused by fluctuations in factors that are often naturally occurring, such as water vapor, clouds, temperature and surface albedo (essentially the brightness or reflectivity of Earth’s surface). The researchers calculated the energy change caused by each of these natural factors, then subtracted the values from the total. The portion leftover is the radiative forcing. The team found that human activities have caused the radiative forcing on Earth to increase by about 0.5 Watts per square meter from 2003 to 2018. The increase is mostly from greenhouse gases emissions from things like power generation, transport and industrial manufacturing. Reduced reflective aerosols are also contributing to the imbalance. NASA Goddard Direct Observations of Forcings See Links to references↩︎ "],["attribution.html", "3 Attribution 3.1 Map of Attribution Studies 3.2 Bottom Trawling CO2 release 3.3 Company Attribution", " 3 Attribution For too long, weather’s randomness has kept events such as these from being blamed squarely on climate change. Reporters in the late 1990s and early 2000s would ask climate scientists about climate change’s role in a weather-related disaster. All we could say was that we’d expect to see more of these events. Now, we can specify increased chances for specific events. This extends to forecasts: we can identify the places that are more likely to see wildfires, mudslides and fish die-offs. Such calculations dent both climate denial and a false sense of security. They take away the argument that ‘extreme weather happens anyway, so we don’t need to worry about it.’ Extreme weather happens — and these metrics pinpoint what is becoming more likely, by how much and why. Betts (2021) Nature Ametsoc (2021) Explaining Extreme Weather (pdf) 3.1 Map of Attribution Studies Known as “extreme event attribution,” the field has gained momentum, not only in the science world, but also in the media and public imagination. These studies have the power to link the seemingly abstract concept of climate change with personal and tangible experiences of the weather. Scientists have published more than 300 peer-reviewed studies looking at weather extremes around the world, from wildfires in Alaska (pdf) and hurricanes in the Caribbean to flooding in France and heatwaves in China. The result is mounting evidence that human activity is raising the risk of some types of extreme weather, especially those linked to heat. To track how the evidence on this fast-moving topic is stacking up, Carbon Brief has mapped – to the best of our knowledge – every extreme-weather attribution study published to date. The map above shows 355 extreme weather events and trends across the globe for which scientists have carried out attribution studies. Carbon BriefMap 3.2 Bottom Trawling CO2 release Time Magzine: How Industrial Fishing Creates More CO2 Emissions Than Air Travel Bottom trawling is responsible for one gigaton of carbon emissions a year—a higher annual total than (pre-pandemic) aviation emissions. Not only does the practice contribute to climate change, it is extremely damaging to ocean biodiversity—the “equivalent of ploughing an old-growth forest into the ground, over and over and over again until there is nothing left” Bottom trawling is also one of the least cost effective methods of fishing. Most locations have been trawled so many times, there is little left worth catching. Without government subsidies, no one would be making a penny. Refuting a long-held view that ocean protection harms fisheries, the study found that well placed marine protected areas (MPAs) that ban fishing would actually boost the production of marine life by functioning as fish nurseries and biodiversity generators capable of seeding stocks elsewhere. Sala Marine sediments are the largest pool of organic carbon on the planet and a crucial reservoir for long-term storage29. If left undisturbed, organic carbon stored in marine sediments can remain there for millen-nia30. However, disturbance of these carbon stores can re-mineralize sed-imentary carbon to CO2, which is likely to increase ocean acidification, reduce the buffering capacityof the ocean and potentially add to the build-up of atmospheric CO2 Using satellite-inferred information on fishing activity by industrial trawlers and dredgers between 2016 and 2019, aggregated at a reso-lution of 1km2, we estimate that 4.9million km2 or 1.3% of the global ocean is trawled each year. This disturbance to the seafloor results in an estimated 1.47Pg of aqueous CO2 emissions, owing to increased carbon metabolism in the sediment in the first year after trawling. If trawling continues in subsequent years, emissions decline as sediment carbon stocks become exhausted. However, after 9 years of continuous trawling, emissions stabilize at around 40% of the first year’s emissions, or around 0.58Pg CO2 (Supplementary Fig.35). If the intensity and footprint of trawling remains constant, we estimate that sediment carbon emissions will continue at approximately 0.58Pg CO2 for up to around 400 years of trawling, after which all of the sediments in the top metre are depleted. Although 1.47Pg CO2 represents only 0.02% of total marine sedimentary carbon, it is equivalent to 15–20% of the atmospheric CO2 absorbed by the ocean each year32,33, and is compara-ble to estimates of carbon loss in terrestrial soils caused by farming34. Although an unknown fraction of the aqueous CO2 is emitted to the atmosphere, the increase in CO2 in the water column and sediment pore waters can have far-reaching and complex effects on marine carbon cycling, primary productivity and biodiversity. Time Magzine BBC Sala (2021) Protecting the global ocean for biodiversity, food and climate - Nature Share 3.3 Company Attribution A 2017 report by the Carbon Disclosure Project showed that 100 companies have been responsible for 71 per cent of global emissions since 1988. In 2019, a similar study from the Climate Accountability Institute found that just 20 companies were responsible for 35 per cent of all energy-related carbon dioxide and methane worldwide since 1965. Sultana "],["carbon-budget.html", "4 Carbon Budget 4.1 Net-zero", " 4 Carbon Budget The temperature response for a 1.5°C scenario has a huge uncertainty &amp; this propagates to the uncertainty in the carbon budget. To say “the remaining carbon budget for 1.5°C is 440 GtCO₂” [add favorite number] is highly misleading Taking a narrow 67–33% range, the value is 230–670 GtCO₂, but full range (left) could be −1000 - 2000 GtCO₂… (yes, could be negative or huge) (Glen Peters) Memo Matthews: The remaining carbon budget quantifies the future CO 2 emissions to limit global warming below a desired level. Carbon budgets are subject to uncertainty in the Transient Climate Response to Cumulative CO 2 Emissions (TCRE), as well as to non-CO 2 climate influences. We estimate a median TCRE of 0.44 °C and 5–95% range of 0.32–0.62 °C per 1000 GtCO 2 emitted. Considering only geophysical uncertainties, our median estimate of the 1.5 °C remaining carbon budget is 440 GtCO 2 from 2020 onwards, with a range of 230–670 GtCO 2 , (for a 67–33% chance of not exceeding the target). Additional socioeconomic uncertainty related to human decisions regarding future non-CO 2 emissions scenarios can further shift the median 1.5 °C remaining carbon budget by ±170 GtCO 2 . Remaining carbon budgets (RCBs) represent the future cumulative CO 2 emissions that would be consistent with keeping global warming to a specified level. Despite being conceptually simple, RCBs have been defined and estimated in various ways and with many different underlying assumptions, resulting in a wide range of “best estimates” across different studies 2 . Moreover, most of these estimates of remaining budgets account for only a subset of the relevant uncertain processes and often omit the contribution of key uncertain processes (such as permafrost thaw or future scenario uncertainty, among others) Median TCRE estimate is 0.44 °C per 1000 GtCO2 , with a 5–95% range of 0.32–0.62 °C per 1000 GtCO2 A stronger constraint on the left-hand side of the distribution (low TCRE values, with sharply increasing probability above 0.25 °C/ 1000 GtCO 2 ), while the right-hand side of this distribution has a wider tail. This right-skewed distribution shape of our observationally-constrained TCRE estimate is physically related to the possibility of a large negative aerosol forcing Median RCB for 1.5 °C is 440 GtCO2 from 2020 onwards, representing a 50% chance of stabilising warming at or below 1.5 °C. The corresponding budget for a 67% chance of remaining below the target is 230 GtCO2 from the year 2020 onwards. Matthews(2021) Carbon Budget Uncertainties (pdf) 4.1 Net-zero Disaster looms if big finance is allowed to game the carbon offsetting markets to achieve ‘net zero’ emissions. Net zero increasingly involves highly questionable carbon accounting. As a result, the new politics swirling around net zero targets is rapidly becoming a confusing and dangerous mix of pragmatism, self-delusion and weapons-grade greenwash. The science of net zero is simple: every sector of every country in the world needs to be, on average, zero emissions. We know how to do this for electricity, cars, buildings and even a lot of heavy industry. But in certain areas, including air travel and some agricultural emissions, there is no prospect of getting to zero emissions in the near future. For these residual emissions, greenhouse gasses will need to be sucked out of the atmosphere at the same rate as they are added, so that, on average, there are net zero emissions. The science of net zero is simple: every sector of every country in the world needs to be, on average, zero emissions. We know how to do this for electricity, cars, buildings and even a lot of heavy industry. But in certain areas, including air travel and some agricultural emissions, there is no prospect of getting to zero emissions in the near future. For these residual emissions, greenhouse gasses will need to be sucked out of the atmosphere at the same rate as they are added, so that, on average, there are net zero emissions. Making this work requires carbon removal, also known as “negative emissions.” This can be low-tech, like restoring forests, as this takes carbon out of the atmosphere and stores it in trees. Or it can be hi-tech, like using chemicals to strip carbon dioxide from the atmosphere and then pumping it deep underground into safe geological storage. In theory this is all fine, as pragmatically some carbon removal is needed to balance hard-to-reduce emissions: but negative emissions and offsetting alone are not a route to net zero. In practice, by believing in the promise of these methods, we are too often deceiving ourselves, in three major ways. The first is an unrealistic overreliance on carbon removal to preserve the status quo. Critically, there is far too little land to plant enough trees to counter today’s emissions, and large-scale hi-tech methods do not yet exist. The second deception is in offsetting against notional emissions trajectories instead of removing carbon from the atmosphere.Offsetting needs to be used to remove carbon dioxide from the atmosphere to counter difficult-to-remove emissions, and not just be an enabler of business-as-nearly-usual. The third deception comes from not getting what you think you’re paying for in the self-regulated global carbon market. The commercial carbon offset concept relies on “additionality” – that money paid then reduces emissions or captures carbon that would not otherwise have happened. The offsets market is awash with old legacy carbon credits where that assumption is violated. If such deceptions remain, disaster looms. Big finance, led by Carney, is planning to massively expand carbon markets. Conceivably, new carbon-based financial products could boom, with little impact on emissions. Just like the sub-prime crisis, few will understand what they bought, and another globe-spanning crash could sweep the world, compounding economic and climate crises causing mass suffering, as we realise again that the Earth owes us nothing. Nature doesn’t do bailouts. Lewis (Guardian) "],["climate-sensitivity.html", "5 Climate Sensitivity 5.1 Climate Feedbacks 5.2 ECS - Equilibrium Climate Sensitivity 5.3 Remote Sensing of Tipping Points", " 5 Climate Sensitivity 5.1 Climate Feedbacks Abstract Heinze Earth system models (ESMs) are key tools for providing climate projections under different sce- narios of human-induced forcing. ESMs include a large number of additional processes and feedbacks such as biogeochemical cycles that traditional physical climate models do not consider. Yet, some processes such as cloud dynamics and ecosystem functional response still have fairly high uncertainties. In this article, we present an overview of climate feedbacks for Earth system components currently included in state-of-the-art ESMs and discuss the challenges to evaluate and quantify them. Uncertainties in feedback quantification arise from the in- terdependencies of biogeochemical matter fluxes and physical properties, the spatial and temporal heterogeneity of processes, and the lack of long-term continuous observational data to constrain them. We present an outlook for promising approaches that can help to quantify and to constrain the large number of feedbacks in ESMs in the future. The target group for this article includes generalists with a background in natural sciences and an interest in climate change as well as experts working in interdisciplinary climate research (researchers, lecturers, and students). This study updates and significantly expands upon the last comprehensive overview of climate feedbacks in ESMs, which was produced 15 years ago (NRC, 2003). Heinze (2019) Climate Feedbacks (pdf) 5.2 ECS - Equilibrium Climate Sensitivity ECS Climate sensitivity is defined as the equilibrium change in global and annual mean surface air temperature, \\(\\Delta T\\), due to an increment in downward radiative flux, \\(\\Delta R_{f}\\) , that would result from sustained doubling of atmospheric \\(CO_2\\) over its preindustrial value (2 x \\(CO_2\\) ). Studies based, on observations, energy balance models, temperature reconstructions, and global climate models (GCMs) have found that the probability density distribution of \\(\\Delta T\\) is peaked in the range 2.0°C ≤ \\(\\Delta T\\) ≤ 4.5°C, with a long tail of small but finite probabilities of very large temperature increases. An important parameter in climate science is the equilibrium or long-run response in the global mean surface temperature to a doubling of atmospheric carbon dioxide. In the climate science community, this is called the equilibrium climate sensitivity ECS. With reference to climate models, this is calculated as the increase in average surface temperature with a doubled CO2 concentration relative to a path with the pre-industrial CO2 concentration. This parameter also plays a key role in the geophysical components in the IAMs. Given the importance of the ECS in climate science, there is an extensive literature estimating probability density functions. These pdfs are generally based on climate models, the instrumental records over the last century or so, paleoclimatic data such as estimated temperature and radiative forcings over ice-age intervals, and the results of volcanic eruptions. Much of the literature estimates a probability density function using a single line of evidence, but a few papers synthesize different studies or different kinds of evidence. The IPCC Fifth Assessment report (AR5) reviewed the literature quantifying uncertainty in the ECS and highlighted five recent papers using multiple lines of evidence (IPCC, 2014). Each paper used a Bayesian approach to update a prior distribution based on previous evidence (the prior evidence usually drawn from instrumental records or a climate model) to calculate the posterior probability density function. Gilligham (2015) Modelling Uncertainty (pdf) 5.2.1 Roe and Baker Distribution Abstract Uncertainties in projections of future climate change have not lessened substantially in past decades. Both models and observations yield broad probability distributions for long-term increases in global mean temperature expected from the doubling of atmospheric carbon dioxide, with small but finite probabilities of very large increases. We show that the shape of these probability distributions is an inevitable and general consequence of the nature of the climate system, and we derive a simple analytic form for the shape that fits recent published distributions very well. We show that the breadth of the distribution and, in particular, the probability of large temperature increases are relatively insensitive to decreases in uncertainties associated with the underlying climate processes. Memo Roe and Baker What determines the distribution shape of ECS and in particular, the high \\(\\Delta T\\) tail? To what extent we can decrease the distribution width? Climate consists of a set of highly coupled, tightly interacting physical processes. Understanding these physical processes is a massive task that will always be subject to uncertainty. How do the uncertainties in the physical processes translate into an uncertainty in climate sensitivity? Explanations for the range of predictions of DT, summarized in (14), have focused on (i) uncertainties in our understanding of the individual physical processes (in particular, those associated with clouds), (ii) complex interactions among the individual processes, and (iii) the chaotic, turbulent nature of the climate system, which may give rise to thresholds, bifurcations, and other discontinuities, and which remains poorly understood on a theoretical level. We show here that the explanation is far more fundamental than any of these. We use the framework of feedback analysis to examine the relationship between the uncertainties in the individual physical processes and the ensuing shape of the probability distribution of \\(\\Delta T\\). Because we are considering an equilibrium temperature rise, we consider only time-independent processes. Roe and Baker (2007) Climate Sensitivity (pdf) Memo Hannart RB addressed these questions using rather simple theoretical considerations and reached the conclusion that reducing uncertainties on climate feedbacks and underlying climate processes will not yield a large reduction in the envelope of climate sensitivity. In this letter, we revisit the premises of this conclusion. We show that it results from a mathematical artifact caused by a peculiar definition of uncertainty used by these authors. Reducing inter-model spread on feedbacks does in fact induce a reduction of uncertainty on climate sensitivity, almost proportionally. Therefore, following Roe and Baker assumptions, climate sensitivity is actually not so unpredictable. The main originality of RB07 approach consists in analyzing explicitly the way uncertainties on \\(f\\), due to a limited understanding of their underlying physical processes, prop- agates into uncertainties on \\(\\Delta T\\): assuming \\(f\\) is a random variable with mean \\(f\\) and standard deviation \\(\\sigma_f\\) , RB07 uses this simple probabilistic model to highlight several fundamental properties of uncertainty propagation from feedbacks to climate sensitivity. The most prominent conclusion of this analysis is that reducing uncertainties on \\(f\\) does not yield a large reduction in the uncertainty of \\(\\Delta T\\), and thus that improvements in the understanding of physical processes will not yield large reductions in the envelope of future climate projections. We show that this conclusion is a mathematical artifact with no connection whatsoever to climate. RB07 uses the feedback analysis framework. Denoting \\(\\Delta T_0\\) the Planck temperature response to the radiative perturbation and \\(f\\) the feedback gain (RB07 refers to it as feedback factor), they obtain: \\[\\Delta T = \\frac{\\Delta T_0}{1 - f}\\] RB07 then assumes uncertainty on Planck response to be negligible so that the entire spread on \\(\\Delta T\\) results from the uncertainty on the global feedback gain \\(f\\). To model this uncertainty, RB07 assumes that \\(f\\) follows a Gaussian distribution with mean \\(\\overline{f}\\) , standard deviation \\(\\sigma_f\\) and implicit truncation for \\(f\\) &gt; 1. Then, they derive an exact expression of the distribution of \\(\\Delta T\\). This simple probabilistic climatic model is used by RB07 to analyze the way uncertainties on \\(f\\), due to a limited understanding of underlying physical processes, propagates into uncertainties on \\(\\Delta T\\). Their analysis highlights two fundamental properties: Amplification: The term in \\(\\frac{1}{1-f}\\) amplifies uncertainty on feedbacks, all the more intensely as \\(f\\) is close to (though lower than) one. Small uncertainties on feedbacks are thus converted in large uncertainties on the rise of temperature. Insensitivity: reducing uncertainty on \\(f\\) has little effect in reducing uncertainty on \\(\\Delta T\\), also stated as the breadth of the distribution of \\(\\Delta T\\) is relatively insensitive to decreases in \\(\\sigma_f\\). We are puzzled by the second property, that is, the claimed insensitivity of uncertainty on \\(\\Delta T\\) to uncertainty on feedbacks. The reason why one may find this second assertion puzzling, is that it intuitively seems to contradict the first. While the probability \\(P(\\Delta T \\in [4.5°C, 8°C])\\) may be of interest practically, this metric is irrelevant to describe the breadth of the distribution of climate sensitivity which was RB07 explicit intent. To address this question, any measure of distribution spread chosen amongst those classically used in Descriptive Statistics is more appropriate. (Hugo Mathjax don’t render correctly here:?? OK in rpad !! OK in mathjaxtest!!) With such measures when the spread of feedback parameter \\(S_f\\) decreases, the resulting spread of climate sensitivity \\(S_{\\Delta T}\\) values also decreases. Further the decrease is approximately linear for \\(S_f\\) small and tends to be steeper for larger values of \\(S_{f}\\) . Hannart on RB (pdf) Tol: RB-fitting Github Memo Jules and James Roe and Baker have attempted to justify the pdfs that have been generated as not only reasonable, but inevitable on theoretical grounds RB’s basic point is that if “feedback” \\(f\\) is considered to be Gaussian, then sensitivity = \\(\\lambda_0/(1-f)\\) is going to be skewed, which seems fair enough. Where I part company with them is when they claim that this gives rise to some fundamental and substantial difficulty in generating more precise estimates of climate sensitivity, and also that it explains the apparent lack of progress in improving on the long-standing 1979 Charney report estimate of 1.5-4.5C at only the “likely” level. Stoat’s complaints also seem pertinent: \\(f\\) cannot really be a true Gaussian, unless one is willing to seriously consider large negative sensitivity, and even though a Gaussian is a widespread and often reasonable distribution, it is hard to find any theoretical or practical basis for a Gaussian abruptly truncated at 1. I can think of several alternative theories as to why the uncertainty in the IPCC estimate has not reduced. The probabilistic methods generally used to generate these long-tailed pdfs are essentially pathological in their use of a uniform prior (under the erroneous belief that this represents “ignorance”), together with only looking at one small subset of the pertinent data at a time, and therefore do not give results that can credibly represent the opinions of informed scientists. There may also be the sociological effect of this range as some sort of anchoring device, which people are reluctant to change despite its rather shaky origins. Ramping up uncertainty (at least at the high end) is a handy lever for those who argue for strong mitigation, and it would also be naive to ignore the fact that scientists working in this area benefit from its prominence. Jules and James: Comment on RB Memo Gillingham Note that the US government used a version of the Roe and Baker distribution calibrated to three constraints from the IPCC for its uncertainty estimates (IAWG, 2010). Specifically, the IAWG Report modified the original Roe and Baker distribution to assume that the median value is 3.0°C, the probability of being between 2 and 4.5°C is two-thirds, and there is no mass below zero or above 10°C. The modified Roe and Baker distribution has a higher mean ECS than any of the models (3.5°C) and a much higher dispersion (1.6°C as compared to 0.84°C from Olsen et al. 2012). Gilligham (2015) Modelling Uncertainty (pdf) 5.2.2 GCM based Approach Gavin (2019) RealClimate (Part 1) Gavin (2020) RealClimate (Part 2) 5.2.3 GCM free Approach Memo GCM free approach The atmosphere is a complex system involving turbulent processes operating over a wide range of scales starting from millimeters at the Kolmogorov dissipation scale up to the size of the Earth, spanning over 10 orders of magni- tudes in space. The dynamics are sensitive to initial conditions and there are deterministic predictability limits that are roughly equal to the eddy turn-over time (lifetime) of structures. For planetary scale structures in the atmosphere, the overall deterministic prediction limit of about 10 days corresponds to the scaling transition timescale τ w from the weather regime to the macroweather regime. The atmospheric components of GCMs exhibit the same weather-macroweather scaling transition as the atmosphere and similar predictability limits. Beyond this horizon, the internal variability has to be interpreted stochastically so that a single GCM run is only one realization of the random process; at these timescales, weather models effectively become stochastic macroweather generators. For projec- tions over multi-decadal timescales and beyond, multi-model ensembles (MME) that include several models are used. The mean of the MME is taken to obtain the deterministic forced component of temperature variability and average out the internal variability (Collins et al. 2013). Emergent properties of the Earth’s climate, i.e. proper- ties which are not specified a priori, are then inferred from GCM simulations. The equilibrium climate sensitivity (ECS) is such a property; it refers to the expected temperature change after an infinitely long time following a doubling in carbon dioxide ( CO 2 ) atmospheric concentration. Another is the transient climate response (TCR), which is defined as the change in temperature after a gradual doubling of CO 2 atmospheric concentration over 70 years at a rate of 1% per year. However, it is not clear whether such emer- gent properties from computational models can be taken as genuine features of the natural world. The difficulty is that each GCM has its own climate (“structural uncertainty”) and this leads to very large discrepancies in ECS and TCR between GCMs; this underscores the need for qualitatively different approaches which can narrow down the properties of the real climate directly from observations. The ecological consequences of global warming could be dire; therefore, better constraining climate sensitivity is of utmost importance in order to meet the urgency of adjusting economical and environmental policies. Multidecadal climate projections rely almost exclusively on deterministic global climate models (GCMs) in spite of the fact that there are still very large structural uncertainties between Coupled Model Intercomparison Project phase 5 (CMIP5) GCMs, i.e. each has its own climate, rather than the real world climate. Climate skeptics have argued that IPCC projections are untrustworthy precisely because they are entirely GCM based. While this conclusion is unwarranted, it underscores the need for independent and qualitatively different approaches. It is therefore significant that the alternative GCM-free approach we present here yields comparable results albeit with smaller uncertainty. According to our projections made to 2100, to avert a 1.5 K warming, future emissions will be required to undergo drastic cuts similar to RCP 2.6, for which we found a 46% probability to remain under the said limit; it is virtually cer- tain that RCP 4.5 and RCP 8.5-like futures would overshoot. Even a 2.0 K warming limit would surely be surpassed by 2100 under RCP 8.5 and probably also under RCP 4.5, with only a 6% chance of remaining under the limit. The safest option remains RCP 2.6 which we project to remain under 2.0 K with very high confidence. The question remains whether it is at all realistic given that it relies strongly on the massive deployment of speculative negative emission technologies. On the other hand, our model has obvious limitations since it assumes a linear stationary relationship between forcing and temperature, neglecting nonlinear interac- tions which could arise as the system evolves, as it cur- rently warms. In particular, so-called tipping points could be reached in the coming century which would lead to a breakdown of the linear model proposed. Such potential behaviours are of critical value for improving future projections, but they have not yet been observed with high confidence even in GCMs. This underlines the need to exploit paleoclimate archives to achieve a better understanding of low-frequency natural variability, namely the transition scale from the macroweather regime to the climate regime. In this study, we have assumed the increased variability in the climate regime to be strictly a result of forcing, but internal modes of variability could also have a significant contribution for longer timescales. Climate News Network Climate Sensitivity article (Climate Dynamics) (pdf) 5.3 Remote Sensing of Tipping Points Many aspects of the climate are sensitive to small disrupting changes that could trigger an abrupt change in the system into a new stable state. Even at relatively low levels of global warming, systems that exhibit these instabilities could accelerate global warming through climate feedbacks or cause other cascading impacts. These ‘tipping elements,’ or ‘large-scale discontinuities in the climate system,’ as UNFCCC IPCC reports refer to them, have been assigned successively greater risk with each IPCC report since 2001. Proximity to a tipping point may be indicated in remote sensing data by characteristic statistical changes. Early warning indicators can be developed using an increasing trend in the lag-1 autocorrelation when it is correlated with an increase in variance. Niklas Boers of the Potsdam Institute for Climate Impact Research highlighted recent work using these characteristic statistical changes to identify the reduction in a system’s resilience, and has developed early warning indicators for Arctic sea-ice extent, Greenland ice sheet, Atlantic Meridional Overturning Circulation, the Amazon rainforest and the South American Monsoon system. The technique has also been applied to aquatic ecosystems and marine anoxic events. Automatic detection of extreme events and abrupt shifts in climate datasets using edge detection algorithms. futureearth "],["decoupling.html", "6 Decoupling", " 6 Decoupling Hausfather Absolute Decoupling of Economic Growth and Emissions in 32 Countries Between 1990 and 2019, global emissions of CO2 increased by 56%. Historically, economic growth has been closely linked to increased energy consumption — and increased CO2 emissions in particular — leading some to argue that a more prosperous world is one that necessarily has more impacts on our natural environment and climate. There is a lively academic debate about our ability to “absolutely decouple” emissions and growth — that is, the extent to which the adoption of clean energy technology can allow emissions to decline while economic growth continues. Over the past 15 years, however, something has begun to change. Rather than a 21st century dominated by coal that energy modelers foresaw, global coal use peaked in 2013 and is now in structural decline. These 32 countries show that it is possible to have economic growth at the same time that CO2 emissions decline, even accounting for embodied emissions in goods imported from overseas. However, these are mostly relatively wealthy countries whose economies tend to be increasingly driven by lower-energy information technology and service sectors. We have relatively few examples of low- or middle-income countries with a focus on energy-intensive manufacturing experiencing absolute decoupling to date. Absolute decoupling is possible. There is no physical law requiring economic growth — and broader increases in human wellbeing — to necessarily be linked to CO2 emissions. All of the services that we rely on today that emit fossil fuels — electricity, transportation, heating, food — can in principle be replaced by near-zero carbon alternatives, though these are more mature in some sectors (electricity, transportation, buildings) than in others (industrial processes, agriculture). Hausfather "],["paleoclimate.html", "7 Paleoclimate 7.1 Holocen Thermal Maxima", " 7 Paleoclimate Earth’s paleoclimate history provides guidance with a precision and reliability that climate models cannot match. Ice cores were usually the paleoclimate data source of choice for those scientists concerned about human-made climate change. That’s understandable because ice cores provide precise data on atmospheric composition as well as climate change. However, ice cores cover only several hundred thousand years. That sounds like a long time, but Earth was mostly in ice ages during that time. The ice cores encompass several interglacial periods, but none of them were much warmer than our present global temperature. (James Hansen: Sophies Planet Ch.40) 7.1 Holocen Thermal Maxima Abstract Bova Proxy reconstructions from marine sediment cores indicate peak temperatures in the first half of the last and current interglacial periods (the thermal maxima of the Holocene epoch, 10,000 to 6,000 years ago, and the last interglacial period, 128,000 to 123,000 years ago) that arguably exceed modern warmth1,2,3. By contrast, climate models simulate monotonic warming throughout both periods4,5,6,7. This substantial model–data discrepancy undermines confidence in both proxy reconstructions and climate models, and inhibits a mechanistic understanding of recent climate change. Here we show that previous global reconstructions of temperature in the Holocene1,2,3 and the last interglacial period8 reflect the evolution of seasonal, rather than annual, temperatures and we develop a method of transforming them to mean annual temperatures. We further demonstrate that global mean annual sea surface temperatures have been steadily increasing since the start of the Holocene (about 12,000 years ago), first in response to retreating ice sheets (12 to 6.5 thousand years ago), and then as a result of rising greenhouse gas concentrations (0.25 ± 0.21 degrees Celsius over the past 6,500 years or so). However, mean annual temperatures during the last interglacial period were stable and warmer than estimates of temperatures during the Holocene, and we attribute this to the near-constant greenhouse gas levels and the reduced extent of ice sheets. We therefore argue that the climate of the Holocene differed from that of the last interglacial period in two ways: first, larger remnant glacial ice sheets acted to cool the early Holocene, and second, rising greenhouse gas levels in the late Holocene warmed the planet. Furthermore, our reconstructions demonstrate that the modern global temperature has exceeded annual levels over the past 12,000 years and probably approaches the warmth of the last interglacial period (128,000 to 115,000 years ago). Bova (2021) Nature (Paywall) "],["pattern-effect.html", "8 Pattern Effect", " 8 Pattern Effect Abtract Our planet’s energy balance is sensitive to spatial inhomogeneities in sea surface temperature and sea ice changes, but this is typically ignored in climate projections. Here, we show the energy budget during recent decades can be closed by combining changes in effective radiative forcing, linear radiative damping and this pattern effect. The pattern effect is of comparable magnitude but opposite sign to Earth’s net energy imbalance in the 2000s, indicating its importance when predicting the future climate on the basis of observations. After the pattern effect is accounted for, the best-estimate value of committed global warming at present-day forcing rises from 1.31 K (0.99–2.33 K, 5th–95th percentile) to over 2 K, and committed warming in 2100 with constant long-lived forcing increases from 1.32 K (0.94–2.03 K) to over 1.5 K, although the magnitude is sensitive to sea surface temperature dataset. Further constraints on the pattern effect are needed to reduce climate projection uncertainty. Nature article (paywall) "],["temperature-measurements.html", "9 Temperature Measurements", " 9 Temperature Measurements Temperatures have increased over virtually the entire planet since the mid-19th century, but the warming rate has not been the same everywhere. When looking at the changes relative to the global average, it is clear the Arctic and land areas are warming faster than the ocean. "],["albedo.html", "10 Albedo", " 10 Albedo Some text on Albedo "],["atmosphere.html", "11 Atmosphere 11.1 Emissions 11.2 Attributing Emissions", " 11 Atmosphere - Emissions - CO2 - Methane - - Attributing Emissions - Norway&#39;s Responsibility 11.1 Emissions Fig: Cumulative Emissions 1751-2018 by Country/Region The UK (like the US) is 5X more responsible for global warming than the average nation. 11.1.1 CO2 11.1.2 Methane Methane, the largest component of natural gas, is sometimes called a “short-lived climate pollutant” because it remains in the atmosphere for far less time than carbon dioxide, which can remain in the atmosphere for hundreds of years. But methane is also a climate “super-pollutant,” 86 times more potent than carbon dioxide at warming the atmosphere over a 20-year period. Sources of methane include wetlands, rice paddies, livestock, biomass burning, organic waste decomposition and fossil fuel drilling and transport. 11.1.2.1 Cut Methane Now Methane is the biggest and really the only lever we have to slow temperature rise during the next two decades. Methane’s potency and short atmospheric life make it a key greenhouse gas for policy makers to focus on as a way to combat global warming in the near term because the impact of those cuts will be felt almost immediately. “If we cut methane emissions substantially during the 2020s, the abundance or concentration in the atmosphere will also drop rapidly during the 2020s,” said Drew Shindell, an earth science professor at Duke University. “If we cut CO2 emissions, it takes a long time for actual concentrations to drop, and then longer for the climate to adjust.” Inside 11.2 Attributing Emissions 11.2.1 Norway’s Responsibility In real life responsibility is more than what is legally binding The emissions of CO2 that occur within Norway’s territory are dwarfed by the emissions that result from combustion of all the oil and gas Norway produces. Because these fossil fuels are exported before being combusted, the emissions are allocated to the accounts of other countries. If Norway had generated electricity from the gas and then exported the electricity, for example, then emissions from that electricity generation would be allocated to Norway’s accounts. There is therefore an element of artificiality associated with this allocation. It takes two to tango. Norway’s territorial emissions of CO2 were about 42 Mt in 2019, and over 1971–2019 totalled about 1.9 Gt. In comparison, emissions from Norwegian oil and gas since 1971 have been about 16 Gt. A similar amount (~15 Gt) will be emitted if all remaining Norwegian oil and gas resources are extracted from the continental shelf. In 2019, emissions from Norwegian oil and gas amounted to 84 tonnes of CO2 for every person in Norway. Norways Export Emissions (Robbie Andrew) In Norwegian politics, there’s been a very successful attempt to separate the discussion of oil policy from the discussion of climate policy. The two were never really tightly linked [in the country] until roughly the last decade, and this division has become increasingly difficult to maintain. Norwegian politicians also haven’t been alone in creating the conditions that made this division possible. They’ve been helped immensely by the international climate regime. From the very beginning of international climate policy, there was this agreement that countries had to account for the emissions that they create when they burn fossil fuels. All the responsibility was placed on the demand side, not the supply side, which was very convenient for Norway. Europe is the primary market for Norway’s oil and gas. But determining the climate effects of Norwegian production is not straightforward. One study has estimated a clear climate benefit from reducing oil output, but the market is complex and the result really depends on your assumptions about how other actors will behave and how the market will evolve over time. The big irony here is that Norway is a fairly large fossil-fuel producer, but we use relatively few fossil fuels directly in our energy use. Nearly all of our electricity has for a long time come from hydropower. In most years, we even export quite a lot of renewable electricity to our neighbors. The only place where fossil fuels are used to directly produce energy is to run the platforms offshore. They use gas to run the turbines to get the energy needed for oil and gas production. The government’s new climate plan, which was unveiled just a few days ago, does include a number of new and more aggressive measures to reduce Norway’s domestic emissions. The proposal to increase the already quite high CO2 tax on offshore emissions came as something of a surprise, and it is likely to pass even if it is currently being challenged by the industry. However, it is important to keep in mind that this proposal only targets the production-related emissions of Norwegian oil, not the level of oil that is being extracted and exported. As such, it is in line with the historical separation between climate and oil policymaking, which tends to focus only on emissions happening within Norway and exclude any concern for the climate impact of exported oil and gas. The Norwegian paradox has worked out fairly well up until the last few years because there has been little focus on the production of fossil fuels, and because Norway is small enough to avoid the scrutiny that some larger nations face. But this is quickly changing, both in the domestic and international political discussion. There is now a lot more focus on the supply side of fossil fuels than 10 years ago, with several countries like Denmark announcing an end to drilling and new research showing a mismatch between planned fossil-fuel production and ideas such as a “non-proliferation treaty” for fossil fuels being floated. The treaty would bring the world together in agreeing to end the use of fossil fuels much like the UN came together to curb the spread of nuclear weapons. This will make it increasingly hard for Norway to hold on to a leadership claim as long as oil production keeps being expanded into new areas. Norways Climate - Petroleum Dilemma (Bard Lahn) 11.2.2 Global North vs South Responsibility The global North is responsible for 92% of total excess carbon dioxide emissions. Climate breakdown is colonial in character and ultimately requires an anti-colonial struggle in response. (Jason Hickel) Fig. by (AndrewFanning?) "],["forests.html", "12 Forests 12.1 Forests tipping: go from sink to source of CO2 due to temperature increase. 12.2 Plant and Cut - Forest CCS 12.3 Deforestation Footprint", " 12 Forests 12.1 Forests tipping: go from sink to source of CO2 due to temperature increase. New research shows that Earth’s overheated climate will alter forests at a global scale even more fundamentally, by flipping a critical greenhouse gas switch in the next few decades. The study suggests that, by 2040, forests will take up only half as much carbon dioxide from the atmosphere as they do now, if global temperatures keep rising at the present pace. Global warming has contributed to thinning canopies in European forests and to sudden die-offs of aspen trees in Colorado, as well as insect outbreaks that are killing trees around the world. In many places, forests are not growing back. The data show a clear temperature limit, above which trees start to exhale more CO2 than they can take in through photosynthesis. The findings mark a tipping point, of sorts, at which “the land system will act to accelerate climate change rather than slow it down,” Trees From Sink to Source (InsideClimateNews) At present, the land provides a “climate service” by absorbing around 30 per cent of the emissions caused by humans each year. Unlike other tipping elements in the Earth system, the climate tipping point for the terrestrial biosphere could be exceptionally close – 20-30 years away – without action. Plant respiration, the process by which plants produce energy for growth, causes CO2 to be released into the atmosphere. The ‘buffer’ or ‘discount’ against carbon emissions that we currently receive from the biosphere is more fragile than we previously realised. Climate models are tools used by scientists to simulate how the world is likely to respond to greenhouse gas emissions. However, it is worth noting that the global dataset used in the study uses “very few samples from tropical regions.” This means that it is still not fully understood how tropical forests are responding to rising temperatures. Independent Memo Duffy The temperature dependence of global photosynthesis and respiration determine land carbon sink strength. While the land sink currently mitigates ~30% of anthropogenic carbon emissions, it is unclear whether this ecosystem service will persist and what hard temperature limits, if any, regulate carbon uptake. The mean temperature of the warmest quarter (3-month period) passed the thermal maximum for photosynthesis during the past decade. At higher temperatures, respiration rates continue to rise in contrast to sharply declining rates of photosynthesis. Under business-as-usual emissions, this divergence elicits a near halving of the land sink strength by as early as 2040. The difference between gross primary productivity and total ecosystem respiration (carbon uptake by vegetation minus carbon loss to the atmosphere) comprises the metabolic component of the land carbon sink [net ecosystem productivity (NEP)]. To date, land ecosystems provide a climate regulation service by absorbing ~30% of anthropogenic emissions annually. While temperature functions as a key driver of year-to-year changes in the land carbon sink, its temperature response is still poorly constrained at biome to global scales, making the carbon consequences of anticipated warming uncertain. Like all biological processes, metabolic rates for photosynthesis and respiration are temperature dependent; they accelerate with in- creasing temperature, reach a maximum rate, and decline thereafter. Highly divergent land carbon sink trajectories from Earth system models. Continued future increases in sink strength due to the CO2 fertilization. The temperature response of global photosynthesis shows distinct maxima at 18°C for C3 and 28°C for C4 plant systems. In contrast to photosynthesis, respiration rates increase across the range of ambient temperatures (up to 38°C), with no evidence of Tmax or rate decline. The thermal maxima of leaf and soil respiration reside at ~60°-70°C. Responses diverge at temperatures above Tmax. The imbalance grows more pronounced as temperature increases. Current climate mostly lies just below Tmax where slight increases in temperature act as climate fertilization of land carbon uptake. Under anticipated warming foreshadowed by historical temperature extremes and coincident land carbon loss—however, more and more time will be spent above Tmax. Past this threshold, the land carbon balance will first weaken and ultimately reverse sign from carbon sink to carbon source. 25°C constitutes a powerful tipping point for the land sink of carbon and a formidable positive climatic feedback, Currently, less than 10% of the terrestrial biosphere experiences where land carbon uptake is degraded. For regions that do experience these temperatures, exposure is limited to 1 to 2 months or constitutes areas with sparse to no vegetation. Under business-as-usual emissions, by 2100, up to half of the terrestrial biosphere could experience temperatures past the treshold. The impact of elevated temperatures on the land sink is more than a function of cumulative area. Biomes that cycle 40 to 70% of all terrestrial carbon including the rainforests of the Amazon and Southeast Asia and the Taiga forests of Russia and Canada are some of the first to exceed biome-specific Tmax for half the year or more. This reduction in land sink strength is effectively front-loaded in that a 45% loss occurs by midcentury, with only an additional 5% loss by the end of the century. These estimates are conservative as they assume full recovery of vegetation after temperature stress and ignore patterns and lags in recovery. In contrast to any CO2 fertilization effect, anticipated higher temperatures associated with elevated CO2 could degrade land carbon uptake. Failure to account for this results in a gross overestimation of climate change mitigation provided by terrestrial vegetation. We are rapidly entering temperature regimes where biosphere productivity will precipitously decline and calls into question the future viability of the land sink. Duffy(2021) Temperature tipping point of the terrestrial biosphere (pdf) 12.2 Plant and Cut - Forest CCS Wood can also serve purely as a long-term carbon storage device. The key to locking away the carbon is to cut off the oxygen supply to microbes, thereby preventing decomposition. Natural experiments show how this can be done. 19th-century lumberjacks in the US and Canada frequently stored logs on the surfaces of the Great Lakes or floated them down rivers, some of which ended up sinking along the way. These have remained in such good condition that a modern-day cottage industry has arisen to recover the logs and turn them into everything from hardwood floors to violins. New Zealand has a similar industry with logs that were fortuitously buried in swamps as long as 60,000 years ago. Based on such examples, scholars have proposed chopping down trees or collecting fallen logs and intentionally stowing them away. That could mean sinking them to the bottom of lakes, interring them in abandoned mines or burying them in specially dug trenches. The idea hasn’t gotten much traction yet, but in 2013, the Quebec Ministry of Agriculture, Fisheries and Food funded a pilot project to dig a trench and bury 35 metric tons of wood. The project came to about $29 per metric ton of CO2 sequestered, according to government scientist Ghislain Poisson, in line with a theoretical estimate of $10-$50. That is cheaper than most high-tech forms of carbon capture and storage, which usually involve machines that filter carbon out of the air and pump it underground. Sequestering carbon at the typical power plant, where emissions are highly concentrated, runs to $30-$91 per metric ton of CO2, but in open air, which is the holy grail, costs theoretically range from $94-$232. To help this promising new technology get off the ground (or rather, into the ground), the federal government offers a tax credit of about $35 for every metric ton of CO2 removed in industrial carbon capture and storage. It’s a policy that has enjoyed strong bipartisan support for over a decade. Plant and Cut (CNN) 12.3 Deforestation Footprint (see: env) Hoang: Mapping Deforestation Footprint](https://www.nature.com/articles/s41559-021-01417-z) "],["ice-sheet.html", "13 Ice Sheet 13.1 Greenland 13.2 Antarctica", " 13 Ice Sheet 13.1 Greenland Abstract Noel Under anticipated future warming, the Greenland ice sheet (GrIS) will pass a threshold when meltwater runoff exceeds the accumulation of snow, resulting in a negative surface mass balance (SMB &lt; 0) and sustained mass loss. Here we dynamically and statistically downscale the outputs of an Earth system model to 1 km resolution to infer that a Greenland near‐surface atmospheric warming of 4.5 ± 0.3 °C—relative to pre‐industrial—is required for GrIS SMB to become persistently negative. Climate models from CMIP5 and CMIP6 translate this regional temperature change to a global warming threshold of 2.7 ± 0.2 °C. Under a high‐end warming scenario, this threshold may be reached around 2055, while for a strong mitigation scenario it will likely not be passed. Depending on the emissions scenario taken, our method estimates a 6‐13 cm sea level rise from GrIS SMB in the year 2100. Noel (2021) Greenland Ice Sheet Loss (pdf) 13.2 Antarctica "],["ocean.html", "14 Ocean 14.1 Sea Level Rise 14.2 AMOC - Gulf Stream", " 14 Ocean 14.1 Sea Level Rise In its most recent assessment, the Intergovernmental Panel on Climate Change said the sea level was unlikely to rise beyond 1.1 metre (3.6ft) by 2100. But climate researchers from the University of Copenhagen’s Niels Bohr Institute believe levels could rise as much as 1.35 metres by 2100, under a worst-case warming scenario. “The models used to base predictions of sea level rise on presently are not sensitive enough,” he said. “To put it plainly, they don’t hit the mark when we compare them to the rate of sea level rise we see when comparing future scenarios with observations going back in time.” Higher Sea Level Rise (Guardian) 14.1.1 TSLS Transient Sea Level Sensitivity By analyzing the mean rate of change in sea level (not sea level itself), we identify a nearly linear rela- tionship with global mean surface temperature (and there- fore accumulated carbon dioxide emissions) both in model projections and in observations on a century scale. This mo- tivates us to define the “transient sea level sensitivity” as the increase in the sea level rate associated with a given warm- ing in units of meters per century per kelvin. We find that future projections estimated on climate model responses fall below extrapolation based on recent observational records. This comparison suggests that the likely upper level of sea level projections in recent IPCC reports would be too low. Sea level projections as assessed in AR5 and SROCC systematically fall below what would be expected from extrapolating observations to warmer conditions, as well as below the expert elicitation. Error bars show estimated likely ranges (17 %–83 %). Grindsted (2021) Transient Sensitivity of Sea Level Rise (Ocean Science) (pdf) 14.2 AMOC - Gulf Stream Weakest Gulf Stream in Millenium The Atlantic Meridional Overturning Circulation (AMOC)—one of Earth’s major ocean circulation systems—redistributes heat on our planet and has a major impact on climate. Here, we compare a variety of published proxy records to reconstruct the evolution of the AMOC since about ad 400. A fairly consistent picture of the AMOC emerges: after a long and relatively stable period, there was an initial weakening starting in the nineteenth century, followed by a second, more rapid, decline in the mid-twentieth century, leading to the weakest state of the AMOC occurring in recent decades. Caesar (2021) AMOC Millenium Weakest (Nature Geoscience) [paywall!] Rahmstorf - Twitter Thread The Guardian The Guardian (Commentary) 14.2.1 The Rise and Fall of AMOC The AMO is simply an artifact of studies that misinterpret the time-varying pattern of human-caused climate change as a low-frequency oscillation At times I feel like I created a monster when I gave a name to this putative climate oscillation in 2000. The concept of the AMO has since been misapplied and misrepresented to explain away just about every climate trend under the sun, often based on flawed statistical methods that don’t properly distinguish a true climate oscillation from a time-varying trend: If you assume that all trends are a simple linear ramp, and call everything left-over an “oscillation,” then the simple fact that global warming flattened out from the 1950s through the 1970s driven by the ramp-up in cooling sulphate aerosol pollution masquerades as an apparent “oscillation” on top of a simple linear trend. We’ve published a number of articles over the years (see e.g. here, here, here, here, here, and here) demonstrating that studies that use such an approach to define the AMO end up mis-attributing to a natural “oscillation” what is actually human-caused climate change. Such analyses have been used by some to dismiss, among other things, the impact climate change is having on increasingly active and destructive Atlantic hurricane seasons, attributing the increase in recent decades to a supposed upturn in the AMO. RealClimate 14.2.2 Rethinking AMOC Chafik A weakened AMOC may have played a role in causing almost 600 years’ worth of frigid winters in Europe and North America. This period, called the Little Ice Age, lasted roughly from 1300 until 1870 and came on the heels of the Medieval Warm Period (circa 950–1250), when temperatures in the Northern Hemisphere were unusually warm. Figure: This simplified view (top) shows the surface flows (red arrows) and deep return flows (blue arrows) that make up the large-scale ocean circulation in the North Atlantic. Color bands on the ocean surface indicate average sea surface temperatures from 1900 to 2019 (data are from the Hadley Centre) and highlight the northward extent of warm waters to higher latitudes. The longitude-depth temperature distribution of the ocean (bottom; data are from the World Ocean Atlas 2018) across the Greenland-Scotland Ridge (GSR, white transect line in the top panel) is also shown. The exchange of waters across the GSR is driven by the rapid loss of heat to the atmosphere over the Nordic Seas. This heat loss causes the waters to sink and build a huge reservoir of cold, dense water that spills back into the deep North Atlantic across the GSR, completing the overturning process. Nearly half of the AMOC’s poleward flow of warm, salty waters enters the Nordic Seas—comprising the Greenland, Iceland, and Norwegian Seas. Here the water cools and pools north of the undersea Greenland-Scotland Ridge (GSR). A host of important questions remains about the dynamics of the ocean near the GSR and the effects of these dynamics on regulating climate. The AMOC has two pathways of overturning circulation. One is open ocean convection in the Irminger and Labrador Seas that produces the upper layer of North Atlantic Deep Water (NADW). The second involves progressive cooling of warm, salty water from the Atlantic in the Nordic Seas. This cooling results in dense water spilling over the GSR back into the North Atlantic—mainly through two passages, the Denmark Strait between Greenland and Iceland and the Faroe Bank Channel south of the Faroes—and forming a lower layer of NADW. Both regions depend upon heat loss to produce water of greater density, but it appears that huge heat losses from the Nordic Seas and the concomitant production and pooling of very dense water behind the GSR are fundamental to maintaining a mild climate in northern Europe. This heat loss produces a healthy supply of NADW that spills back into the global abyss and enables warm, salty water to feed the Nordic Seas. Evidence of strong variability in Nordic Seas inflow on multidecadal timescales. The volume of and heat transported in this poleward flow, as measured at the GSR, are strongly coupled to the Atlantic multidecadal variability (AMV), which describes natural patterns of sea surface temperature variability in the North Atlantic that influence climate globally The AMV affects Nordic Seas inflow because deep convection in the northeast Atlantic translates the surface temperature variations down into the upper layers of the ocean, and these variations shape the ocean’s dynamic height field. The inflow of warm water to the Nordic Seas has been quite stable over the past century since the start of modern oceanography. Nordic Seas overturning circulation has been stable over the past 100 years. This stability is surprising given the extraordinary warming presently underway in the Nordic Seas and Arctic Ocean. The continued stability of this vital ocean circulation system is not guaranteed in the future. It is also unclear how future change may manifest or which early-warning indicators should be relied upon to forecast change. The recent discovery of an unknown route by which cold water courses its way through the Norwegian Sea. We identified that this new route directs cold deep flows north of the Faroe Islands to the Norwegian slope before turning them south through the Faroe-Shetland Channel and into the deep North Atlantic. Which route water takes north of the GSR and how much is funneled each way depend on the prevailing winds. Under weak westerly wind conditions in the Nordic Seas, the densest water that feeds the Faroe Bank Channel comes primarily from north of Iceland. During strong westerly wind conditions, however, more water seems to originate from along the Jan Mayen Ridge, which is located farther north of Iceland and more in the middle of the Nordic Seas. This wind dependence is curious, considering the strong control that bathymetry can exert on the circulation. Deep rapid flow, or deep jet, called the Faroe-Shetland Channel Jet. Remarkably, this jet flows south along the eastern slope of the channel rather than along the western side as has long been assumed. The deep jet is found to be the main current branch in terms of transport that delivers the densest water to the North Atlantic Ocean via the Faroe Bank Channel. This surprising finding countered past observations and thinking. We do not yet have a firm grasp of the deep circulation of the Nordic Seas and how it varies over time. All available observational evidence so far indicates that there is no long-term trend in the Nordic Seas meridional overturning circulation to date. The degree to which fresh water from the Arctic and Greenland Sea can mix with and dilute warm, saline water from the Atlantic. Such dilution could suppress deep temperature- and density-driven convection, thus weakening or shutting down the overturning in the Nordic Seas and, by extension, the deepest component of the AMOC. However, most scientists no longer think such a shutdown scenario is likely because observations to date indicate that Arctic and Greenland waters tend to remain trapped around and south of Greenland rather than mixing and diluting the Atlantic water flowing north in the Nordic Seas Nonetheless, there is broad agreement that the climatic consequences of a potential shutdown of this vital ocean circulation are so enormous that they obligate us to improve our understanding of the Nordic Seas. Chafik (2021) Rethinking Oceanic Overturning in Nordic Seas "],["permafrost.html", "15 Permafrost", " 15 Permafrost Some text on Permafrost "],["soil.html", "16 Soil", " 16 Soil Guardian The storage potential of one of the Earth’s biggest carbon sinks – soils – may have been overestimated, research shows. This could mean ecosystems on land soaking up less of humanity’s emissions than expected, and more rapid global heating. The study, based on over 100 experiments, found the opposite. When plant growth increases, soil carbon does not. The finding is significant because the amount of organic carbon stored in soils is about three times that in living plants and double that in the atmosphere. Soils can also store carbon for centuries, whereas plants and trees rot quickly after they die. When rising CO2 increases plant growth, there is a decrease in soil carbon storage. If soils do absorb less in future, “the speed of global warming could be higher” Soils, plants and trees are important for carbon levels, but ending the burning of fossil fuels is essential. To stop global warming, we need to stop emissions, because ecosystems only take up a fraction of all the CO2 emissions. The researchers found that in grasslands, elevated CO2 led to 9% plant growth – less than forests – but soil carbon rose by 8%. Terrier said there has been a lot of discussion about tree planting as a way to tackle the climate crisis. “What I found very concerning in that debate is that people were suggesting planting trees in natural grasslands, savannah, and tundra,” he said. “I think that would be a terrible mistake because, as our results imply, there is a very large potential to increase soil carbon storage in grasslands.” Given that the land absorbs 30% of the carbon emitted from fossil fuels and deforestation, understanding if that will change in the future matters. Change would be determined by the balance between rising CO2 boosting plant growth and the negative effects of climate change itself, including drought, heatwaves and fires. The evidence to date suggests the biggest change will be the negative effects of global heating on ecosystems Guardian "],["adaptation.html", "17 Adaptation 17.1 Lagging mitigation, lagging adaptation", " 17 Adaptation Climate Services may help build resilience Climate Services Norsk KlimaServiceSenter 17.1 Lagging mitigation, lagging adaptation Given the current uncertainties around efforts to limit climate change, the world must plan for, finance and implement climate change adaption measures appropriate for the full range of global temperature increases or face serious costs, losses and damages. Adaptation – reducing countries’ and communities’ vulnerability to climate change by increasing their ability to absorb impacts and remain resilient – is a key pillar of the Paris Agreement. The Agreement requires all of its signatories to plan and implement adaptation measures through national adaptation plans, studies, monitoring of climate change effects and investment in a green future. The Gap Report finds that such action is lagging far behind where it should be. It finds that while nations have advanced in planning and implementation, huge gaps remain, particularly in finance for developing countries and bringing adaptation projects to the stage where they bring real reductions in climate risks. The Green Climate Fund (GCF) has allocated 40 per cent of its total portfolio to adaptation and is increasingly crowding-in private sector investment. Another important development is the increasing momentum to ensure a sustainable financial system. New tools such as sustainability investment criteria, climate-related disclosure principles and mainstreaming of climate-related risks into investment decisions can stimulate investments in climate resilience and direct finance away from investments that increase vulnerability. Nature-based solutions (NbS), one of the most cost-effective ways in the adaptation portfolio, has a potential to make a big contribution to climate change adaptation, but there are few tangible plans and limited financing available for them. NbS are mainly used to address coastal hazards, intense precipitation, heat and drought. UNEP Adaptation Report 2020 "],["carbon-pricing.html", "18 Carbon Pricing 18.1 Policy Sequencing 18.2 Limited Impact on Emissions 18.3 Carbon Price Norway", " 18 Carbon Pricing 18.1 Policy Sequencing Meckling Many economists have long held that carbon pricing—either through a carbon tax or cap-and-trade—is the most cost-effective way to decarbonize energy systems, along with subsidies for basic research and development. Meanwhile, green innovation and industrial policies aimed at fostering low-carbon energy technologies have proliferated widely. Most of these predate direct carbon pricing. Low-carbon leaders such as California and the European Union (EU) have followed a distinct policy sequence that helps overcome some of the political challenges facing low-carbon policy by building economic interest groups in support of decarbonization and reducing the cost of technologies required for emissions reductions. However, while politically effective, this policy pathway faces significant challenges to environmental and cost effectiveness, including excess rent capture and lock-in. Here we discuss options for addressing these challenges under political constraints. As countries move toward deeper emissions cuts, combining and sequencing policies will prove critical to avoid environmental, economic, and political dead-ends in decarbonizing energy systems. The EU offers a prime example of this policy sequence. The EU adopted rules on promoting renewable energies in 2001, after eight member states had already implemented renewable-energy sup-port schemes. This occurred in the context of the liberalization of electricity markets across Europe. The EU followed up with indus-trial policy for renewable fuels in the transport sector in 2003.In a second phase, the EU adopted carbon pricing in 2003, which entered into force in 2005.In a third phase, the EU’s decarbonization efforts led to a ratcheting up of all measures in the 2020 Climate and Energy Package of 2009, the 2030 Climate and Energy Package of 2014, and the recent EU winter package of 2016. California followed a path similar to that of the EU11. China is on its way to replicate the policy path of climate leaders: in the mid-2000s, it adopted supply-side industrial policy to develop clean-energy industries, followed by feed-in tariffs that fostered domestic demand for renewable energy, leading to a domestic carbon pricing system in the energy sector to be implemented in 2017 Careful policy sequencing can help facilitate the progressive decarbonization of energy systems under political constraints, as California and the EU demonstrate. An excessive focus on the need for efficient pricing alone often ignores these constraints. A better integration of economic and political perspectives should help point the way forward on low-carbon policymaking Wagner (Blog) Meckling (Nature)(web-pdf) 18.2 Limited Impact on Emissions Green -Abstract Carbon pricing has been hailed as an essential component of any sensible climate policy. Internalize the externalities, the logic goes, and polluters will change their behavior. The theory is elegant, but has carbon pricing worked in practice? Despite a voluminous literature on the topic, there are surprisingly few works that conduct an ex-post analysis, examining how carbon pricing has actually performed. This paper provides a meta-review of ex-post quantitative evaluations of carbon pricing policies around the world since 1990. Four findings stand out. First, though carbon pricing has dominated many political discussions of climate change, only 37 studies assess the actual effects of the policy on emissions reductions, and the vast majority of these are focused on Europe. Second, the majority of studies suggest that the aggregate reductions from carbon pricing on emissions are limited – generally between 0% and 2% per year. However, there is considerable variation across sectors. Third, in general, carbon taxes perform better than emissions trading schemes (ETSs). Finally, studies of the EU-ETS, the oldest emissions trading scheme, indicate limited average annual reductions – ranging from 0% to 1.5% per annum. For comparison, the IPCC states that emissions must fall by 45% below 2010 levels by 2030 in order to limit warming to 1.5 degrees Celsius – the goal set by the Paris Agreement (IPCC 2018). Overall, the evidence indicates that carbon pricing has a limited impact on emissions. \"Green -Memo* Carbon taxes place a surcharge on fuel or energy use. In emissions trading schemes, the government sets a ceiling or cap on the total amount of allowed emissions. Allowances are distributed to those firms regulated by the scheme, either free of charge or by auction. Each firm then has the right to emit up to its share of allowances. They may also trade allowances with each other to meet their individual emission allocations. Those who emit more than their allowance can purchase more; those that emit less can sell their excess supply, or bank it for future use. an Carbon taxes and ETSs differ in a number of respects. First, carbon taxes provide certainty of cost: the price is set by the government. Yet there is no limit on emissions, provided that regulated entities are willing and able to pay the tax. By contrast, ETSs provide certainty of quantity: the cap, set by the government, constitutes the upper limit on emissions. The cost will vary, depending on the scarcity (or oversupply) of allowances, and other design features. In practice, the distinction between the two policies is sometimes blurred (Hepburn, 2006). For example, an ETS might have a floor price; this guaranteed price makes it resemble a tax. First, the mismatch between the incremental effects of carbon pricing and the demand for rapid decarbonization cannot be understated. The IPCC states that emissions must fall by 45% below 2010 levels by 2030 in order to limit warming to 1.5 degrees Celsius – the goal set by the Paris Agreement (IPCC 2018). The Low Carbon Economy Index estimates that this translates to an annual emissions reduction of 11.3% by the “average” G20 nation (PwC 2019). Yet GHG emissions have risen an average of 1.5% per year in the last decade (UN Environment 2019, p. iv). It is important to understand the extent to which one of the most widely-used climate policies contributes to this goal. Second, there is little evidence to suggest that carbon pricing promotes decarbonization The most common outcome is fuel-switching and efficiency improvements. Unlike policies which create pathways to decarbonization – such as binding renewable portfolio standards, feed in tariffs or investment in R&amp;D – carbon pricing addresses emissions (flow), rather than overall concentrations of greenhouse gases (stock). The real work of emission control is done through regulatory instruments. Within the EU, where nations are also part of the EU-ETS, nations without a carbon tax reduced emissions more quickly than those with a carbon tax. It is astonishing how little hard evidence there is on the actual performance of carbon pricing policies using ex-post data. Tthe overall effect on reductions for both types of policy is quite small, generally between 0-2% per annum. Norway, Sweden and Denmark were early adopters, implementing some of the first carbon taxes in 1991-92. EU-ETS was the first compulsory emissions trading scheme, beginning in 2005. The single study of California cap and trade scheme estimates that between 24%-43% of emissions from electricity generation were shifted out of state to avoid carbon pricing regulations. The drivers of these modest reductions are incremental solutions: fuel switching, enhanced efficiency, and reduced consumption of fuels. These actions, though useful on the margins, fall well short of the societal transformations identified needed. A common rejoinder is that carbon prices simply aren’t high enough to generate substantial emissions reductions. Indeed, low prices are pervasive; the vast majority of carbon prices are well below even the most conservative estimates of the “social cost of carbon” (SCC). Given the prevalence of low prices, it is particularly important to consider the few jurisdictions with carbon prices at or near the SCC. Sweden has the highest carbon price in the world. Studies range in their reduction estimates from 0%-17% per year, with the upward bound being an outlier among all 37 studies. In 2019, Finnish taxes on transport fuels were at $68 per ton, and $58 per ton for all other fossil fuels. Emissions reductions there are estimated to be between 0%-1.7% The other two jurisdictions with high carbon taxes are Switzerland ($99 per ton in 2019) and Lichtenstein ($99 per ton in 2019 , No estimates of their effects on emissions). It may be the case that pricing will work better after a certain threshold is surpassed. Indeed, Aydin and Esen find that energy taxes, including CO2 taxes, only reduce emissions after surpassing 2.2% of GDP (2018). Yet after nearly four decades of experience with carbon pricing, the empirical evidence to date suggests that low prices are a feature of this policy, rather than a bug. More worrisome is the fact that even those nations with high prices have relatively modest reductions. A problem for carbon pricing concerns leakage, which occurs when economic activity subject to carbon pricing shifts to a jurisdiction without similar regulations. This problem is pervasive in environmental regulation, driven by variation in policy stringency. To the extent that leakage occurs, but is excluded from the studies examined here, emissions reductions may be overestimated. Offsets can have two possible impacts on overall reductions. First, to the extent that offsets are not additional, their use will decrease the actual reductions achieved through a carbon pricing policy. To date, offsets have been an important component of most ETSs. Green(2021) Carbon Pricing Ex-Post (pdf) 18.3 Carbon Price Norway Norway has a carbon price covering ~80% of emissions, but it varies substantially by sector. The highest price is ~80€/tCO₂ (domestic aviation) &amp; agriculture is not taxed. The average is ~60€/tCO₂, but Norwegian emissions are dropping very slowly. St.meld. 14 (2020-2021) "],["carbon-offsets.html", "19 Carbon Offsets 19.1 Market Upscaling", " 19 Carbon Offsets Carbon offset schemes allow individuals and companies to invest in environmental projects around the world in order to balance out their own carbon footprints. The projects are usually based in developing countries and most commonly are designed to reduce future emissions. This might involve rolling out clean energy technologies or purchasing and ripping up carbon credits from an emissions trading scheme. Other schemes work by soaking up CO2 directly from the air through the planting of trees. Selling Indulgencies* George Monbiot famously compared carbon offsets with the ancient Catholic church’s practice of selling indulgences: absolution from sins and reduced time in purgatory in return for financial donations to the church. Just as indulgences allowed the rich to feel better about sinful behaviour without actually changing their ways, carbon offsets allow us to buy complacency, political apathy and self-satisfaction. Additionality the key issue for anyone who does want to offset is whether the scheme you’re funding actually achieves the carbon savings promised. This boils down not just to the effectiveness of the project at soaking up CO2 or avoiding future emissions. Effectiveness is important but not enough. You also need to be sure that the carbon savings are additional to any savings which might have happened anyway. The problem is that it’s almost impossible to prove additionality with absolute certainly, as no one can be sure what will happen in the future, or what would have happened if the project had never existed. Partly because of the difficulty of ensuring additionality, many offset providers guarantee their emissions savings. This way, if the emissions savings don’t come through or they turn out to be “non-additional,” the provider promises to make up the loss via another project. As the offset market grows, some offset companies have enough capital to invest in projects speculatively: they fund an offset project and then sell the carbon savings once the cuts have actually been made. This avoids the difficulty of predicting the future – and also avoids the claim that a carbon cut made some years in the future is worth less than a cut made now. These kinds of guarantees and policies provide some reassurances, but do they mean anything in the real world? Without actually visiting the offset projects ourselves, how can individuals be sure that the projects are functioning as they should? Even if offset projects do work as advertised, some environmentalists argue that they’re still a bad idea. If we’re to tackle climate change, they argue, the projects being rolled out by offset companies should be happening anyway, funded by governments around the world, while companies and individuals reduce their carbon footprints directly. Only in this way – by doing everything possible to make reductions everywhere, rather than polluting in one place and offsetting in another – does the world have a good chance of avoiding runaway climate change, such critics claim. Market Standards To try and answer these questions, the voluntary offset market has developed various standards, which are a bit like the certification systems used for fairly traded or organic food. These include the Voluntary Gold Standard (VGS) and the Voluntary Carbon Standard (VCS). Offsets with these standards offer extra credibility, but that still doesn’t make them watertight. Heather Rogers, author of Green Gone Wrong, visited a number of offset schemes in India and found all kinds of irregularities. Offset Price Many people are confused by the low prices of carbon offsets. If it’s so bad for the environment to fly, can a few pounds really be enough to counteract the impact? The answer is that, at present, there are all kinds of ways to reduce emissions very inexpensively. After all, a single low-energy lightbulb, available for just £1 or so, can over the space of six years save 250kg of CO2 – equivalent to a short flight. That’s not to say that offsetting is necessarily valid, or that plugging in a low-energy lightbulb makes up for flying. The point is simply that the world is full of inexpensive ways to reduce emissions. In theory, if enough people started offsetting, or if governments started acting seriously to tackle global warming, then the price of offsets would gradually rise, as the low-hanging fruit of emissions savings – the easiest and cheapest “quick wins” – would get used up. Another frequent point of confusion about the cost of offsetting is that different offset companies quote different prices for offsetting the same activity. There are two reasons for this. First, there are various ways of estimating the precise impact on climate change of certain types of activity – including flying, which affects global temperature in various different ways. Second, different types of offset project will inevitably have different costs – especially given that projects may be chosen not just for the CO2 impacts but for their broader social benefits. Duncan Clark (Guardian 2011): Complete Guide to Carbon Offsetting 19.1 Market Upscaling Carney presented plans at the virtual Davos meeting of global business and political leaders on Wednesday evening for vast increases in the number of carbon offsets sold, aiming to expand the market from about $300m at present to between $50bn and $100bn a year. He told the conference that he “categorically rejected” criticism that offsets were greenwash. Companies buying offsets in the market would be subject to scrutiny, and must have clear plans to reach net zero, “not something written on the back of a napkin,” he said, but would need offsets to fulfil their plans. “This is bringing those companies into a formal system,” he said. “This is about maximising the use of a very limited [global] carbon budget. This is complementary [to companies taking action to reduce their own emissions] and is one piece of the puzzle. We do need this market.” The leaders of two UK environmental charities have written to Mark Carney, the UN climate envoy and former governor of the Bank of England, to raise concerns over the blueprint for carbon offsetting that could result in billions of new carbon credits being sold around the world. Campaigners say system risks becoming greenwashing exercise unless loopholes closed. The markets are used as a cover by companies that wish to give the appearance of working towards net zero emissions but prefer buying cheap credits to the more difficult task of cutting their emissio This initiative risks setting a terrible example ahead of the critical carbon market negotiations at the global climate summit in Glasgow later this year. [It] seems to have ignored past failures of offsetting schemes to guarantee emission cuts. At the same time, it assumes that the natural world has unlimited potential to absorb climate-wrecking emissions. It fails to acknowledge that the most important thing companies must do is to reduce their own emissions and use of fossil fuels. Carney’s scheme will serve as a giant get-out-of-jail-free card for polluting companies. There is a danger that it becomes a large international greenwashing exercise, creating a market with low standards but high PR value, Guardian on Carney Offsets If you scale a bad thing, it doesn’t matter how big you make it, it’s still bad. As Mark Carney presented the world’s top political and business leaders with a blueprint to scale up the voluntary carbon market on Wednesday, campaigners warned key criteria to improve environmental integrity were missing. With carbon neutrality pledges becoming the benchmark for climate ambition, businesses around the world are looking to offset the emissions they cannot cut and for cheaper ways to meet their climate goals. Its report identified ways to bring coherence to what is currently a fragmented market. But it said little on how to ensure projects financed through the market deliver genuinely additional emissions reductions. An open letter to Carney signed by 47 researchers, academics and campaigners ahead of the launch accused the initiative of trying to “minimise the cost of compliance for private corporations” at the cost of environmental integrity. An independent process driven by civil society should define what constitute a quality offset to avoid the private sector self-regulating. Sequestrating carbon in ecosystems should not replace real emissions cuts. We need to sequester past emissions that are already in the atmosphere whereas offsetting by definition is about future emissions. The whole point of an offset is that an entity keeps emitting. There is a striking lack of binding and credible measures to actually prioritise emissions reductions. The biodiversity co-benefits that result from the voluntary carbon market are “a strong assumption” with no evidence. It’s important not to forget the huge flows of finance currently facilitating the drivers of biodiversity loss - warning against the market being perceived as an adequate substitute to conservation. ClimateChangeNews Memo TSVCM Report Chaired by Bill Winters, group chief executive of Standard Chartered, and sponsored by the Institute of International Finance (IIF), the taskforce includes some of the world’s most polluting companies: airline easyJet, plane manufacturer Boeing, oil giants BP, Shell and Total, and steel producer Tata Steel. No green groups are represented among its members. The need for climate action, and tools to mobilize finance for the low-carbon and resilient transition, grows more urgent by the day. To achieve the Paris goals to limit global warming to 1.5 degrees Celsius, the global community needs to reach net-zero emissions by no later than 2050. This will require a whole-economy transition—every company, every bank, every insurer and investor will have to adjust their business models, develop credible plans for the transition, and implement them. Many companies, especially in hard-to- abate sectors, will need to offset emissions as they achieve their decarbonization goals, creating a surge in demand for credible offsets. To facilitate this global decarbonization there is a need for a large, transparent, verifiable and robust voluntary carbon market, one that promotes genuine action of high environmental integrity. Along with the carbon avoided, reduced, or removed, the scaling up of markets has the further potential to help support financial flows to the Global South, as activities and projects in these countries can provide a cost- effective source of these carbon-emission reductions. Voluntary carbon markets can also play a critical role in scaling down cost curves for emerging climate technologies, bringing these technologies to market earlier, and allowing them to be used in direct decarbonization efforts. The Taskforce has found six key areas where efforts are required to achieve a large, transparent, verifiable, and robust voluntary carbon market; these themes are establishing core carbon principles, core carbon reference contracts, infrastructure, offset legitimacy, market integrity, and demand signaling. !! Direct emissions reductions by corporates must be the priority, with appropriate offsetting playing an important complementary role to accelerate climate mitigation action. Taskforce on Scaling Voluntary Carbon Markets (pdf) "],["css-carbon-capture-and-storage.html", "20 CSS - Carbon Capture and Storage 20.1 CSS and DAC cause more Damage than Good 20.2 BECCS", " 20 CSS - Carbon Capture and Storage Tyndall Report: CCS, as the technology is known, is designed to strip out carbon dioxide from the exhaust gases of industrial processes. These include gas- and coal-fired electricity generating plants, steel-making, and industries including the conversion of natural gas to hydrogen, so that the gas can then be re-classified as a clean fuel. The CO2 that is removed is converted into a liquid and pumped underground into geological formations that can be sealed for generations to prevent the carbon escaping back into the atmosphere. It is a complex and expensive process, and many of the schemes proposed in the 1990s have been abandoned as too expensive or too technically difficult. Currently (2020) there are only 26 CCS plants operating globally, capturing about 0.1% of the annual global emissions from fossil fuels. Ironically, 81% of the carbon captured to date has been used to extract more oil from existing wells by pumping the captured carbon into the ground to force more oil out. This means that captured carbon is being used to extract oil that would otherwise have had to be left in the ground. CCS features prominently in many energy and climate change scenarios, and in strategies for meeting climate change mitigation targets. It is the cumulative emissions from each year between now and 2030 that will determine whether we are to achieve the Paris 1.5°C goal. With carbon budgets increasingly constrained we cannot expect CSS to make a meaningful contribution to 2030 climate targets. 20.0.1 CSS will not work as planned and is a dangerous distraction Instead of financing a technology they can neither develop in time nor make to work as claimed, governments should concentrate on scaling up proven technologies like renewable energies and energy efficiency. The technology has not lived up to expectations. Instead of capturing up to 95% of the carbon from any industrial process, rates have been as low as 65% when they begin and have only gradually improved. CCS won’t work (ClimateNewsNetwork) Global operational CCS capacity is currently 39MtCO2 per year, this is about 0.1% of annual global emissions from fossil fuels. There are just 26 operational CCS plants in the world, with 81% of carbon captured to date used to extract more oil via the process of Enhanced Oil Recovery [EOR], and at this stage CCS planned deployment remains dominated by EOR. Financing of these CCS projects has relied on the increased revenue from EOR, CCS is not capable of operating with zero emissions. Many projections assume a capture rate for CCS of 95%, however, capture rates at that level are unproven in practice. Current capacity in the energy sector is just 2.4 MtCO 2 a year. This compares to the International Energy Agency’s (IEA) estimate of 310 MtCO 2 a year in the energy sector by 2030, an increase of 129 times from today. Reliance on CCS is not a solution to the climate emergency. Tyndall Centre Report (pdf Summary) (pdf Report) But the industry do not agree: When CCS was first touted, it was seen as a way of cleaning up electricity generated by fossil fuels, in particular those burning coal. But now it is clear it can play a key role in cleaning up other industries. The opposition to CCS technology from some campaigners seems driven by a hatred of fossil fuel companies that is preventing a level-headed understanding of how we can stop climate change. Industry Response to Tyndall Report 20.1 CSS and DAC cause more Damage than Good Spending money on carbon capture and storage or use (CCS/U) and synthetic direct air capture and storage and use (SDACCS/U) increases carbon dioxide equivalent (CO2e) emissions, air pollution, and costs relative to spending the same money on clean, renewable electricity replacing fossil or biofuel combustion. The low net capture rates are due to uncaptured combustion emissions from natural gas used to power the equipment, uncaptured upstream emissions, and, in the case of CCU, uncaptured coal combustion emissions. Moreover, the CCU and SDACCU plants both increase air pollution and total social costs relative to no capture. Using wind to power the equipment reduces CO2e relative to using natural gas but still allows air pollution emissions to continue and increases the total social cost relative to no carbon capture. Conversely, using wind to displace coal without capturing carbon reduces CO2e, air pollution, and total social cost substantially. Further, using wind to displace coal reduces more CO2e than using the same wind to power the capture equipment. As such, spending money on wind powering carbon capture always increases CO2e compared with spending on the same wind replacing fossil fuels or biofuels. In sum, CCU and SDACCU increase or hold constant air pollution health damage and reduce little carbon before even considering sequestration or use leakages of carbon back to the air. Spending on capture rather than wind replacing either fossil fuels or bioenergy always increases CO2e, air pollution, and total social cost substantially. No improvement in CCU or SDACCU equipment can change this conclusion while fossil power plant emissions exist, since carbon capture always incurs an equipment cost never incurred by wind, and carbon capture never reduces, instead mostly increases, air pollution and fuel mining, which wind eliminates. Once fossil power plant emissions end, CCU (for industry) and SDACCU social costs need to be evaluated against the social costs of natural reforestation and reducing nonenergy halogen, nitrous oxide, methane, and biomass burning emissions. Jakobsen (2019) 20.2 BECCS BECCS, rapidly emerged as the new saviour technology. By burning “replaceable” biomass such as wood, crops, and agricultural waste instead of coal in power stations, and then capturing the carbon dioxide from the power station chimney and storing it underground, BECCS could produce electricity at the same time as removing carbon dioxide from the atmosphere. That’s because as biomass such as trees grow, they suck in carbon dioxide from the atmosphere. By planting trees and other bioenergy crops and storing carbon dioxide released when they are burnt, more carbon could be removed from the atmosphere. BECCS, just like all the previous solutions, was too good to be true. Across the scenarios produced by the Intergovernmental Panel on Climate Change (IPCC) with a 66% or better chance of limiting temperature increase to 1.5°C, BECCS would need to remove 12 billion tonnes of carbon dioxide each year. BECCS at this scale would require massive planting schemes for trees and bioenergy crops. The Earth certainly needs more trees. Humanity has cut down some three trillion since we first started farming some 13,000 years ago. But rather than allow ecosystems to recover from human impacts and forests to regrow, BECCS generally refers to dedicated industrial-scale plantations regularly harvested for bioenergy rather than carbon stored away in forest trunks, roots and soils. Currently, the two most efficient biofuels are sugarcane for bioethanol and palm oil for biodiesel – both grown in the tropics. Endless rows of such fast growing monoculture trees or other bioenergy crops harvested at frequent intervals devastate biodiversity. It has been estimated that BECCS would demand between 0.4 and 1.2 billion hectares of land. That’s 25% to 80% of all the land currently under cultivation. How will that be achieved at the same time as feeding 8-10 billion people around the middle of the century or without destroying native vegetation and biodiversity? Growing billions of trees would consume vast amounts of water – in some places where people are already thirsty. Increasing forest cover in higher latitudes can have an overall warming effect because replacing grassland or fields with forests means the land surface becomes darker. This darker land absorbs more energy from the Sun and so temperatures rise. Focusing on developing vast plantations in poorer tropical nations comes with real risks of people being driven off their lands. And it is often forgotten that trees and the land in general already soak up and store away vast amounts of carbon through what is called the natural terrestrial carbon sink. Interfering with it could both disrupt the sink and lead to double accounting. Dyke Youtube: BECCS Explainer "],["sai-stratospheric-aerosol-injection.html", "21 SAI Stratospheric Aerosol Injection", " 21 SAI Stratospheric Aerosol Injection Pain-killer - no solution I often compare SAI to painkillers, opioids, which treat symptoms but don’t solve underlying problems – and their use can delay much needed action on a solution. This corresponds to my largest concern about the risks of SAI research: the moral hazard. This is the idea that humanity will relax its already feeble efforts on mitigation because of the hope of a ‘painkiller‘. Also similar to painkillers, there is the danger of addiction to SAI, in which continued use requires higher and higher doses as no action on emissions is taken. Just like sudden withdrawal of opioids can be disastrous, so could sudden cessation of SAI – creating a ‘termination shock,’ as the full force of underlying climate change came to bear over the 2-3 year lifetime of stratospheric aerosol. SAI has not been researched nearly enough to even consider anything close to deployment and research is slow. There are essentially no existing governance processes specific to small-scale outdoor experiments on SAI. As the principal investigator of the Stratospheric Controlled Perturbation Experiment (SCoPEx) – a proposed experiment that would be the first to inject particles into the stratosphere, but which would begin outdoor testing without particle injection – I share concerns about SAI research. What is known, and not known, about stratospheric aerosol injection (SAI)? SAI is the idea of introducing particles into the stratosphere, which would scatter sunlight to space and cool the Earth’s surface. Our prediction is that they would stay there for 2-3 years before returning to ground. Current computer-based models make injecting particles into the atmosphere look good – I fear too good. Some simulations have suggested that combining SAI with emissions cuts could reduce many global climate impacts, such as extreme temperatures, droughts, and tropical storm intensity. SAI could perhaps be used to slow down the rate of climate change (a rapid rate of change could make it impossible for humanity and ecosystems to adapt), while the slower acting – but more important – emission reductions, and very likely carbon dioxide removal, are taking place. Yet, there are also tremendous uncertainties and potential risks around SAI. For example, sulfate aerosol, a commonly modelled aerosol due to its natural stratospheric occurrence, destroys the stratospheric ozone layer and heats up the stratosphere, changing atmospheric circulation. We do not understand stratosphere circulation well enough, and so we cannot know the implications of messing with it without research. SCoPex If we receive permission, our Stratospheric Controlled Perturbation Experiment (SCoPEx) would launch a large research balloon from Esrange Space Center in Kiruna, Sweden. It would rise to approximately 20 kilometers, where we would test its navigation, communication, and instrumentation under extreme stratospheric conditions. Later, if approved by the independent Advisory Committee, follow-up experiments would release about 2 kilograms of calcium carbonate particles over an area roughly 1 kilometer long and 100 meters wide. (SCoPEx proposes to study calcium carbonate as early indoor research suggests it may cause lower ozone destruction and significantly less stratospheric heating than sulfate.) The balloon would then turn around to measure whether the particle sizes, locally reflected sunlight, and chemical impact on ozone match our models. These results and meteorological measurements would then be integrated into global climate models, improving their ability to predict the effectiveness and risks of stratospheric geoengineering. Keutsch "],["cap-and-trade.html", "22 Cap and Trade", " 22 Cap and Trade "],["fee-and-dividend.html", "23 Fee and dividend", " 23 Fee and dividend We agreed on the merits of fee-and-dividend, because it is more efficacious than cap-and-trade and easier to make near-global. Also, it would be popular because it helps address a growing wealth disparity that exists in many nations. We even talked about writing a joint paper on fee-and-dividend to help decisionmakers understand its merits. [James Hansen: Sophie’s Planet Ch.47] "],["sai-stratospheric-aerosol-injection-1.html", "24 SAI Stratospheric Aerosol Injection", " 24 SAI Stratospheric Aerosol Injection As the mirage of each magical technical solution disappears, another equally unworkable alternative pops up to take its place. The next is already on the horizon – and it’s even more ghastly. Once we realise net zero will not happen in time or even at all, geoengineering – the deliberate and large scale intervention in the Earth’s climate system – will probably be invoked as the solution to limit temperature increases. Dyke Pain-killer - no solution I often compare SAI to painkillers, opioids, which treat symptoms but don’t solve underlying problems – and their use can delay much needed action on a solution. This corresponds to my largest concern about the risks of SAI research: the moral hazard. This is the idea that humanity will relax its already feeble efforts on mitigation because of the hope of a ‘painkiller‘. Also similar to painkillers, there is the danger of addiction to SAI, in which continued use requires higher and higher doses as no action on emissions is taken. Just like sudden withdrawal of opioids can be disastrous, so could sudden cessation of SAI – creating a ‘termination shock,’ as the full force of underlying climate change came to bear over the 2-3 year lifetime of stratospheric aerosol. SAI has not been researched nearly enough to even consider anything close to deployment and research is slow. There are essentially no existing governance processes specific to small-scale outdoor experiments on SAI. As the principal investigator of the Stratospheric Controlled Perturbation Experiment (SCoPEx) – a proposed experiment that would be the first to inject particles into the stratosphere, but which would begin outdoor testing without particle injection – I share concerns about SAI research. What is known, and not known, about stratospheric aerosol injection (SAI)? SAI is the idea of introducing particles into the stratosphere, which would scatter sunlight to space and cool the Earth’s surface. Our prediction is that they would stay there for 2-3 years before returning to ground. Current computer-based models make injecting particles into the atmosphere look good – I fear too good. Some simulations have suggested that combining SAI with emissions cuts could reduce many global climate impacts, such as extreme temperatures, droughts, and tropical storm intensity. SAI could perhaps be used to slow down the rate of climate change (a rapid rate of change could make it impossible for humanity and ecosystems to adapt), while the slower acting – but more important – emission reductions, and very likely carbon dioxide removal, are taking place. Yet, there are also tremendous uncertainties and potential risks around SAI. For example, sulfate aerosol, a commonly modelled aerosol due to its natural stratospheric occurrence, destroys the stratospheric ozone layer and heats up the stratosphere, changing atmospheric circulation. We do not understand stratosphere circulation well enough, and so we cannot know the implications of messing with it without research. SCoPex If we receive permission, our Stratospheric Controlled Perturbation Experiment (SCoPEx) would launch a large research balloon from Esrange Space Center in Kiruna, Sweden. It would rise to approximately 20 kilometers, where we would test its navigation, communication, and instrumentation under extreme stratospheric conditions. Later, if approved by the independent Advisory Committee, follow-up experiments would release about 2 kilograms of calcium carbonate particles over an area roughly 1 kilometer long and 100 meters wide. (SCoPEx proposes to study calcium carbonate as early indoor research suggests it may cause lower ozone destruction and significantly less stratospheric heating than sulfate.) The balloon would then turn around to measure whether the particle sizes, locally reflected sunlight, and chemical impact on ozone match our models. These results and meteorological measurements would then be integrated into global climate models, improving their ability to predict the effectiveness and risks of stratospheric geoengineering. Keutsch "],["scc-social-costs-of-carbon.html", "25 SCC - Social Costs of Carbon 25.1 The most important number you have never heard of 25.2 Current Prices far below SCC 25.3 Stern Stiglitz Alternative Approach", " 25 SCC - Social Costs of Carbon The most important number you have never heard of Greenstone Testimony Greenstone Estimating SCC Greenstone Updating SCC Current prices far below SCC SCC: The straw that stirs the drink The dollar value that future populations and governments around the world would be willing to pay to avoid experiencing climate change. If you could see dimly into the future—which you can!—and calculate in dollars the benefits of actions preventing utter destruction and chaos, what would you call this price? You might call it “the alpha price,” the price of having a safe future. The s̶o̶c̶i̶a̶l̶ ̶c̶o̶s̶t̶ ̶o̶f̶ ̶c̶a̶r̶b̶o̶n̶ alpha price is a regulatory tool (yawn) that addresses an important disparity, which is the difference between market prices for fossil fuels and the value of the damage they inflict on the world. By calculating the alpha price… and using it as a “benefit” in cost-benefit analyses, government can make sure that they truly understand the costs and benefits of new policies. They ensure that demonstrably incorrect market prices for fossil fuels are not obscuring the real benefits of action. t’s a complicated thing, calculating the alpha price, which brings us to the question of how bureaucratic inertia and professional conventions perpetuate thinking we don’t need anymore. Hold your breath, we’re free-diving now deep into the bowels of regulatory bureaucracy. Which brings us to the punchline: Is there a better way to bring about a net-zero, safe future without even relying on an alpha price? Is applying a cost-benefit analysis to climate change itself a part of the bureaucratic and professional conventions that are holding back climate progress? Maybe! That’s why(I think!) (noahqk?) et al have given us Near-Term to Net-Zero and (I think) why (GernotWagner?) et al have given us declining CO2 price paths. (Eric Roston, twitter thread). 25.1 The most important number you have never heard of 25.1.1 Greenstone Testimony The social cost of carbon (SCC) is the cost to society of polluting an additional ton of CO2. The SCC enables regulators to account for potential benefits to society through lower carbon emissions and also points towards the optimal price on carbon required to address excess greenhouse gas emissions. The US government SCC was approximately $50 per ton of CO2 as of 2016, using a discount rate of 3%. Since its inception, the SCC has been used in roughly 150 federal regulations that cover energy efficiency, forest conservation, fuel-economy standards, and emissions performance standards. Indeed, in many cases the SCC was instrumental in passing these regulations, offering relevant agencies a reliable, transparent tool to calculate the full benefits the new rules would offer to society. All told, a recent paper calculated, federal regulations written to include the SCC in the US have more than $1 trillion of benefits. When one considers the possibility of larger-than- expected temperature changes for a given change in emissions, sea level rise in short time periods, physical “tipping points,” and human responses like mass migration, then the case for a low discount rate appears strong. The broader point is that global interest rates have declined since the SCC was set and, even setting aside the risk characteristics of payoffs from climate mitigation investments, there is a solid case that the discount rates currently used to calculate the SCC may be too high. When the US accounts for the full global benefits of reducing our emissions, this incentivizes reciprocal climate policies in other countries, like China and India, that reduces their emissions which benefits the US. The use of a SCC that only considers domestic benefits is very likely to deprive the United States of emissions reductions in the United States that would protect us from more virulent climate change Climate Impact Lab (CIL) aims to produce the world’s first empirically derived estimate of the social cost of carbon CIL’s core findings to date have been in the mortality sector. Climate change has a demonstrable impact on mortality rates, as extreme temperatures, both hot and cold, affect health outcomes such as heat stroke and cardiovascular disease. Using data from forty countries and statistical methods to account for the benefits and costs of adaptation, we estimate the full mortality risk due to climate change to be an additional 85 deaths per 100,000 in 2100. This increase in the global mortality rate is more than the mortality rate associated with all infectious diseases in 2018. The elevated mortality risk equates to a monetary cost of $23.6 per metric ton of carbon emitted today when using the same assumptions that underlie the Obama SCC calculation. In other words, we estimate the partial social cost of carbon, accounting for costs to human mortality alone, to be at least $23.6 per ton. The Obama administration’s estimate of the SCC assumed that society is risk neutral, that is, we are not willing to pay a premium to avoid uncertainty. If the more realistic assumption that society is risk averse were introduced, then the estimated SCC would be higher, likely substantially so, than the $50 per metric ton of CO 2 discussed in the previous section. Similarly, there is a good case for scaling the damages from the CIL’s research upwards to reflect the risk aversion that characterizes individuals’ choices in their own lives. While Accra, Ghana will see an increase in the full mortality risk of 160 per 100,000 due to climate change in 2100, Oslo, Norway will experience a decline in the full mortality risk of 230 per 100,000 due to warmer winters. Climate change will leave some regions as winners and others as losers both around the globe and within the United States. Climate impacts vary considerably across locations. While northern latitudes will experience net savings due to reduced heating needs, increased cooling demand will lead to large increases in many areas of the tropics. Within the United States, I find that rural areas will be hard-hit. My co-author and I estimate that with a high emissions scenario 18 , rising temperatures will reduce 2050 corn acreage by 94% and soybean acreage by 98% when compared to levels in 2002. The lost corn and soybean production will not be replaced by increased production of any crop currently grown in the US: as returns to agriculture decline in connection with climate change, farmers will seek to shift land use toward different crops and non-agricultural options. It will effectively bring an end to the more than 150-year tradition of farming as we know it in the US corn-belt that encompasses great swatches of Iowa, Nebraska, and Illinois. The uneven distribution of climate damages means that the very mitigation of climate change is a pursuit of environmental justice. Michael Greenstone testimony (pdf) 25.1.2 Greenstone Estimating SCC (MIT CEEPR WP 2011-006):* For 2010, the central value of the SCC is $21 per ton of CO 2 emissions and sensitivity analyses are to be conducted at $5, $35, and $65 (2007$). This paper summarizes the methodology and process used to develop the SCC values. The 2009-2010 interagency process that developed these SCC values was the first U.S. federal government effort to promote consistency in the way that agencies calculate the social benefits of reducing CO 2 emissions in regulatory impact analyses. 4 Prior to 2008, reductions in CO 2 emissions were not valued in federal benefit–cost analyses. IAMs Analysts face a number of significant challenges when attempting to quantify the economic impacts of CO 2 emissions. In particular, analysts must make assumptions about four main steps of the estimation process: (1) the future emissions of greenhouse gases; (2) the effects of past and future emissions on the climate system; (3) the impact of changes in climate on the physical and biological environment; and, (4) the translation of these environmental impacts into economic damages. Integrated assessment models (IAMs) have been developed to combine these steps into a single modeling framework; the word “integrated” refers to the fact that they integrate knowledge from science and economics. However, they gain this advantage at the expense of a more detailed representation of the underlying climatic and economic systems. The IAMs translate emissions into changes in atmospheric greenhouse gas concentrations, atmospheric concentrations into changes in temperature, and changes in temperature into economic damages. The emissions projections used in the models are based on specified socio- economic (GDP and population) pathways. These emissions are translated into concentrations using the carbon cycle built into each model, and concentrations are translated into warming based on each model’s simplified representation of the climate and a key parameter, climate sensitivity. Finally, transforming the stream of economic damages over time into a single value requires judgments about how to discount them. (DICE-PAGE-FUND differences discussed in paper omitted here) Overall, the power of the IAMs is that they offer guidance to the incredibly complex question of what an extra ton of greenhouse damages will do to human wellbeing. This is no small task and this is what makes them so appealing. However, the results are highly dependent on a series of assumptions that cannot easily be verified. On Discount Rates: Historically Observed Interest Rates Ramsey Equation Ramsey discounting also provides a useful framework to inform the choice of a discount rate. Under this approach, the analyst applies either positive or normative judgments in selecting values for the key parameters of the Ramsey equation: η (coefficient of relative risk aversion or elasticity of the marginal utility of consumption) and ρ (pure rate of time preference). 18 These are then combined with g (growth rate of per-capita consumption) to equal the interest rate at which future monetized damages are discounted: ρ + η·g. Most papers in the climate change literature adopt values for η in the range of 0.5 to 3, although not all authors articulate whether their choice is based on prescriptive or descriptive reasoning. 19 Dasgupta (2008) argues that η should be greater than 1 and may be as high as 3, since η equal to 1 suggests savings rates that do not conform to observed behavior. With respect to the pure rate of time preference, most papers in the climate change literature adopt values for ρ in the range of 0 to 3 percent per year. The very low rates tend to follow from moral judgments involving intergenerational neutrality. Some have argued that to use any value other than ρ = 0 would unjustly discriminate against future generations (e.g., Arrow et al. 1996, Stern et al. 2006). However, even in an inter-generational setting, it may make sense to use a small positive pure rate of time preference because of the small probability of unforeseen cataclysmic events (Stern et al. 2006). Some economists and non-economists have argued for constant discount rates below 2 percent based on the prescriptive approach. When grounded in the Ramsey framework, proponents of this approach have argued that a ρ of zero avoids giving preferential treatment to one generation over another. The choice of η has also been posed as an ethical choice linked to the value of an additional dollar in poorer countries compared to wealthier ones. Stern et al. (2006) applies this perspective through his choice of ρ = 0.1 percent per year η = 1, yielding an annual discount rate of 1.4 percent when combined with the growth rate. Recently, Stern (2008) revisited the values used in Stern et al. (2006), stating that there is a case to be made for raising η due to the amount of weight lower values place on damages far in the future (over 90 percent of expected damages occur after 2200 with η = 1). If there is a persistent element to the uncertainty in the discount rate (e.g., the rate follows a random walk), then it will result in an effective (or certainty-equivalent) discount rate that declines over time. Consequently, lower discount rates tend to dominate over the very long term Updating 2021 - $125 - Abstract This paper outlines a two-step process to return the United States government’s Social Cost of Carbon (SCC) to the frontier of economics and climate science. The first step is to implement the original 2009-2010 Inter-agency Working Group (IWG) framework using a discount rate of 2%. This can be done immediately and will result in an SCC for 2020 of $125. The second step is to reconvene a new IWG tasked with comprehensively updating the SCC over the course of several months that would involve the integration of multiple recent advances in economics and science. We detail these advances here and provide recommendations on their integration into a new SCC estimation framework. Michael Greenstone (2011) Estimatiung SCC (pdf) 25.1.3 Greenstone: Updating SCC In many respects, the SCC is the “straw that stirs the drink” for most domestic climate policies, determining in some cases whether or not regulatory action can proceed. A defining feature of the best new (after 2010) research is that it relies on large-scale data sets, rather than assumptions that are often unverifiable. New estimates for two of three recently studied SCC sectors (mortality and agriculture) indicate substantially larger damages from CO 2 , suggesting that the SCC, as settled in 2013, is too low. Climate change is projected to disproportionately harm today’s poorest populations, exacerbating concerns about environmental justice. The Biden administration can initiate the first step immediately and simply involves implementing the IWG’s approach again with a discount rate of no higher than 2 percent, which reflects profound changes in international capital markets that make the current values difficult to justify. At a discount rate of 2% the SCC in 2020 is $125. There are seven “ingredients” necessary to construct the SCC. The first four are often referred to as “modules” : 1. A socioeconomic and emissions trajectory, which predicts how the global economy and CO 2 emissions will grow in the future; 2. A climate module, which measures the effect of emissions on the climate; 3. A damages module, which translates changes in climate to